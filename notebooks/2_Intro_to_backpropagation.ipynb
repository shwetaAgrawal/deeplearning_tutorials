{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LixRLTwzMBsZ"
      },
      "source": [
        "The idea behind this notebook series is to go slow and build a solid understanding of code and maths involved in deep learning. Few points to note -\n",
        "1. For simplicity of calculations, I have used small integer values in examples\n",
        "2. This code is only for understanding the concepts so it is missing couple of things like type checking and error handling\n",
        "\n",
        "This is the 2<sup>nd</sup> notebook in the series. Here we build upon the content of 1st Notebook and build understanding of backpropagation and gradient descent. \n",
        "\n",
        "This time, I have also added the code to validate our learnings and implementations against PyTorch. There is one major difference is that PyTorch uses row vector for representing weights, inputs and outputs whereas we have been using column vector. This doesn't impact much just at the initialization we need to transpose our vectors before feeding into pytorch networks.\n",
        "\n",
        "[Colab link](https://colab.research.google.com/github/shwetaAgrawal/deeplearning_tutorials/blob/main/notebooks/2_Intro_to_backpropagation.ipynb)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Pre-requisite for running this notebook - numpy. If you come across error \"No module named 'numpy'\" then please uncomment the below line and run this cell\n",
        "#!pip install numpy torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "osEjQ7iSmFKp"
      },
      "source": [
        "# Introduction to Backpropagation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*Backpropagation*, short for \"backward propagation of errors,\" is a fundamental algorithm used in training artificial neural networks. \n",
        "\n",
        "Neural network training can be divided into two phases:\n",
        "1. Forward Pass: During the forward pass, the input data is passed through the network layer by layer to generate an output. Already covered in 1st notebook in the series.\n",
        "2. Backward Pass: In the backward pass, we compare the predicted output and actual output to compute the error. The error is then propagated back through the network. We will cover this in detail in this notebook.\n",
        "\n",
        "Steps in Backpropagation:\n",
        "1. Compute Error: Calculate the error between the predicted output and the actual target values using a loss function (e.g., Mean Squared Error, Cross-Entropy Loss).\n",
        "2. Compute Gradient: Compute the gradients of the error with respect to each weight using the chain rule.\n",
        "3. Update Weights: Adjust the weights and biases using the computed gradients and an optimization algorithm.\n",
        "\n",
        "If you want to do it by hand before jumping into the code, check [this workbook](https://aibyhand.substack.com/p/7-can-you-calculate-a-transformer) published by Tom Yeh."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Importing the custom modules. these are available in repo - https://github.com/shwetaAgrawal/deeplearning_tutorials/tree/main/notebooks\n",
        "from ann_module import ActivationFunctions\n",
        "from ann_module import NeuralNetworkForwardPass\n",
        "from utils import SampleInputOutputUtils"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Backpropagation for single neuron\n",
        "Just like what we did in 1st notebook, we'll start with a very simple example. \n",
        "\n",
        "To recap, our single neuron's calculation\n",
        "$$\n",
        "\\hat y = f(W^T \\cdot x + b)\n",
        "$$\n",
        "where ${f}$ is the activation function, ${W^T}$ is weight matrix and ${b}$ is the bias vector \n",
        "\n",
        "\n",
        "Now we need to define the following: \n",
        "1. Error - It is defined as difference between actual and predicted output - $(y - \\hat y)$\n",
        "2. Loss function (${L}$) - Lets use mean squared error as loss function which is calculated as \n",
        "$$\n",
        "L = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n",
        "$$\n",
        "Where:\n",
        "- $y_i$ is the actual output for i<sup>th</sup> input.\n",
        "- $\\hat{y}_i$ is the predicted output for i<sup>th</sup> input.\n",
        "3. Gradient ($\\frac{\\partial L}{\\partial w}$ and $\\frac{\\partial L}{\\partial b}$) - These are partial derivatives ${L}$ with respect to w and b. \n",
        "4. Learning rate (**η**) - Learning rate is used to tweak update magnitude, it takes value from (0, 1]. High learning rate leads to larger weight updates and increases the chance of missing global minima and weights not coverging. Small learning rate leads to smaller weight updates and increases the chance of being stuck in local minima and increased training time. \n",
        "$$\n",
        "W = W - η \\cdot \\frac{\\partial L}{\\partial w}\n",
        "$$\n",
        "$$\n",
        "b = b - η \\cdot \\frac{\\partial L}{\\partial b}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Loss Calculation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "def computeLoss(y: np.ndarray, y_pred: np.ndarray) -> float:\n",
        "    \"\"\"\n",
        "    This function computes the loss between the predicted value and the actual value.\n",
        "    Args:\n",
        "    \"\"\"\n",
        "    if y is None or y_pred is None or y.size == 0 or y_pred.size == 0:\n",
        "        raise ValueError(\"y and y_pred cannot be None/empty\")\n",
        "    \n",
        "    if y.size != y_pred.size:\n",
        "        raise ValueError(\"y and y_pred should be of same length\")\n",
        "    \n",
        "    return np.mean((y - y_pred) ** 2) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Actual:  [-1 -1  0 -1 -1]\n",
            "\n",
            "Predicted:  [ 0 -1 -1  0 -1]\n",
            "\n",
            "Loss:  0.6\n"
          ]
        }
      ],
      "source": [
        "y = np.random.randint(-1, 1, 5)\n",
        "y_pred = np.random.randint(-1, 1, 5)\n",
        "\n",
        "print(\"\\nActual: \", y)\n",
        "print(\"\\nPredicted: \", y_pred)\n",
        "print(\"\\nLoss: \", computeLoss(y, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Loss using PyTorch:  tensor(0.6000)\n"
          ]
        }
      ],
      "source": [
        "# validate the loss using PyTorch\n",
        "criterion = nn.MSELoss()\n",
        "loss = criterion(torch.tensor(y_pred, dtype=torch.float32), torch.tensor(y, dtype=torch.float32))\n",
        "print(\"\\nLoss using PyTorch: \", loss)\n",
        "\n",
        "assert(loss == computeLoss(y, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Gradient Calculation\n",
        "\n",
        "To start with calculate loss and its derivative for 1 sample i.e our dataset consists of only 1 record and thus has one 1 pair of $y$ and $\\hat{y}$ - \n",
        "$$\n",
        "L =  (y - \\hat{y})^2\n",
        "$$\n",
        "\n",
        "$\\hat{y}$ is a function of $w$ and $b$ as shared earlier, so we can substitute $\\hat{y}$ in loss function with that - \n",
        "$$\n",
        "L = (y - (W^T \\cdot x + b))^2\n",
        "$$\n",
        "\n",
        "As since w and b are independent variables and not impacted by each other, so \n",
        "$$\n",
        "\\frac{\\partial w}{\\partial b} = \\frac{\\partial b}{\\partial w} = 0\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Gradient for weight updation \n",
        "Now we are going to calculate partial derivative of substituted loss function wrt ${w}$ -\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial W} = \\frac{\\partial}{\\partial W} (y - (W^T \\cdot x + b))^2\n",
        "$$\n",
        "\n",
        "Applying the chain rule:\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial W} = 2 (y -(W^T \\cdot x + b)) \\cdot \\frac{\\partial}{\\partial W} (y - (W^T \\cdot x + b))\n",
        "$$\n",
        "\n",
        "Differentiating the inner term (since $y$ is a constant and $\\frac{\\partial b}{\\partial W}$ = 0):\n",
        "$$\n",
        "\\frac{\\partial}{\\partial W} (y - (W^T \\cdot x + b)) = -x\n",
        "$$\n",
        "\n",
        "Combining the results:\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial W} = 2 (y - (W^T \\cdot x + b)) \\cdot (-x) = -2x (y - (W^T \\cdot x + b)) = -2x(y - \\hat{y})\n",
        "$$\n",
        "\n",
        "Now, we can extend above equation to multiple record dataset by taking average of these individual sample derivatives:\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial W} = -\\frac{2}{n} \\sum_{i=1}^{n}x_i(y_i - \\hat{y}_i))\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# lets write down the gradient calculation function now\n",
        "def computeWeightGradient(y: np.ndarray, y_pred: np.ndarray, x_input: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    This function computes the gradient of the loss function with respect to the predicted value.\n",
        "    Args:\n",
        "    \"\"\"\n",
        "    if y is None or y_pred is None or x_input is None or y.size == 0 or y_pred.size == 0 or x_input.size == 0:\n",
        "        raise ValueError(\"y and y_pred cannot be None/empty\")\n",
        "    \n",
        "    if y.size != y_pred.size:\n",
        "        raise ValueError(\"y, y_pred should be of same length\")\n",
        "    if y.size != x_input.shape[-1]:\n",
        "        raise ValueError(\"y and x_input should have same number of records\")\n",
        "    \n",
        "    # we want element wise multiplication of the two arrays here because for a batch of inputs, \n",
        "    # we are multiplying each input with the corresponding error\n",
        "    axis_avg = (x_input.ndim - 1) if x_input.ndim > 1 else None #last axis represents batch of inputs and we want to average over the batch\n",
        "    return -2 * np.mean((y - y_pred) * x_input, axis=axis_avg, keepdims=True) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Gradient for bias updation\n",
        "Similarly we can calculate gradient for bias. \n",
        "$$\n",
        "\\frac{\\partial L}{\\partial b} = \\frac{\\partial}{\\partial b} (y - (W^T \\cdot x + b))^2\n",
        "$$\n",
        "\n",
        "Applying chain rule, we get:\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial b} = 2 (y -(W^T \\cdot x + b)) \\cdot \\frac{\\partial}{\\partial b} (y - (W^T \\cdot x + b))\n",
        "$$\n",
        "\n",
        "Differentiating the inner term (since $y$ is a constant and $\\frac{\\partial w}{\\partial b}$ = 0):\n",
        "$$\n",
        "\\frac{\\partial}{\\partial b} (y - (W^T \\cdot x + b)) = -1\n",
        "$$\n",
        "\n",
        "Combining the results:\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial b} = 2 (y - (W^T \\cdot x + b)) \\cdot (-1) = 2(y - (W^T \\cdot x + b)) = -2(y - \\hat{y})\n",
        "$$\n",
        "\n",
        "Now, we can extend above equation to multiple record dataset by taking average of these individual sample derivatives:\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial b} = \\frac{-2}{n} \\sum_{i=1}^{n}(y_i - \\hat{y}_i)\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# lets write down the gradient calculation function now\n",
        "def computeBiasGradient(y: np.ndarray, y_pred: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    This function computes the gradient of the loss function with respect to the predicted value.\n",
        "    Args:\n",
        "    \"\"\"\n",
        "    if y is None or y_pred is None or y.size == 0 or y_pred.size == 0:\n",
        "        raise ValueError(\"y and y_pred cannot be None/empty\")\n",
        "    \n",
        "    if y.size != y_pred.size:\n",
        "        raise ValueError(\"y and y_pred should be of same length\")\n",
        "    \n",
        "    return -2 * np.mean(y - y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Input: \n",
            " [[4 0 0 1 0]]\n",
            "\n",
            "Target output:  [[1 0 0 1 0]]\n",
            "\n",
            "Weights:  [[-1]]\n",
            "\n",
            "Bias:  [[0]]\n"
          ]
        }
      ],
      "source": [
        "# lets generate some sample inputs and outputs\n",
        "\n",
        "# We'll start with one input and output dimension & a simpler network of just 1 layer mapping input to output\n",
        "input_dimensions = 1\n",
        "output_dimensions = 1 \n",
        "batch_size = 5\n",
        "input_generator = SampleInputOutputUtils(input_dimensions, output_dimensions)\n",
        "x_input, y = input_generator.getSampleInputOutputBatch(batch_size)\n",
        "\n",
        "print(\"\\nInput: \\n\", x_input)\n",
        "print(\"\\nTarget output: \", y)\n",
        "\n",
        "# lets initialize our basic neural network with just 1 layer mapping inputs to outputs\n",
        "nn_custom = NeuralNetworkForwardPass(input_dimensions, output_dimensions, ActivationFunctions.linear, [])\n",
        "\n",
        "# For debugging purposes, lets print the weights and bias of the network\n",
        "print(\"\\nWeights: \", nn_custom.layers[0].weights)\n",
        "print(\"\\nBias: \", nn_custom.layers[0].bias)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Predicted output:  [[-4  0  0 -1  0]]\n",
            "\n",
            "Loss:  5.8\n",
            "\n",
            "Weight Gradient:  [[-8.8]]\n",
            "\n",
            "Bias Gradient:  -2.8\n",
            "\n",
            "Updated Weights:  [[-0.12]]\n",
            "\n",
            "Updated Bias:  [[0.28]]\n"
          ]
        }
      ],
      "source": [
        "y_pred = nn_custom.forward(x_input)\n",
        "\n",
        "print(\"\\nPredicted output: \", y_pred)\n",
        "print(\"\\nLoss: \", computeLoss(y, y_pred))\n",
        "\n",
        "print(\"\\nWeight Gradient: \", computeWeightGradient(y, y_pred, x_input))\n",
        "print(\"\\nBias Gradient: \", computeBiasGradient(y, y_pred))\n",
        "\n",
        "# storing these for validation with PyTorch\n",
        "w0, b0 = nn_custom.layers[0].weights, nn_custom.layers[0].bias\n",
        "w0_grad, b0_grad = computeWeightGradient(y, y_pred, x_input), computeBiasGradient(y, y_pred)\n",
        "\n",
        "# lets update the weights using the gradients\n",
        "learning_rate = 0.1\n",
        "nn_custom.layers[0].weights = nn_custom.layers[0].weights - learning_rate * w0_grad\n",
        "print(\"\\nUpdated Weights: \", nn_custom.layers[0].weights)\n",
        "\n",
        "# lets update the bias using the gradients\n",
        "nn_custom.layers[0].bias = nn_custom.layers[0].bias - learning_rate * b0_grad\n",
        "print(\"\\nUpdated Bias: \", nn_custom.layers[0].bias)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Predicted output using PyTorch:  tensor([[-4.],\n",
            "        [ 0.],\n",
            "        [ 0.],\n",
            "        [-1.],\n",
            "        [ 0.]], grad_fn=<AddmmBackward0>)\n",
            "\n",
            "Loss using PyTorch:  tensor(5.8000, grad_fn=<MseLossBackward0>)\n",
            "\n",
            "Weight Gradient using PyTorch:  tensor([[-8.8000]])\n",
            "\n",
            "Bias Gradient using PyTorch:  tensor([[-2.8000]])\n",
            "\n",
            "Updated Weights using PyTorch:  tensor([[-0.1200]])\n",
            "\n",
            "Updated Bias using PyTorch:  tensor([[0.2800]])\n"
          ]
        }
      ],
      "source": [
        "# validate above gradient calculations using PyTorch\n",
        "\n",
        "# lets convert the numpy arrays to torch tensors\n",
        "# pytorch uses row vectors instead of column vectors, so we need to transpose everything, input, output, weights and bias\n",
        "x_input_torch = torch.tensor(x_input.T, dtype=torch.float32)\n",
        "y_torch = torch.tensor(y.T, dtype=torch.float32)\n",
        "\n",
        "# lets initialize our basic neural network with just 1 layer mapping inputs to outputs\n",
        "nn_torch = nn.Linear(input_dimensions, output_dimensions, bias=True)\n",
        "nn_torch.weight.data = torch.tensor(w0.T, dtype=torch.float32)\n",
        "nn_torch.bias.data = torch.tensor(b0.T, dtype=torch.float32)\n",
        "\n",
        "y_pred_torch = nn_torch(x_input_torch)\n",
        "print(\"\\nPredicted output using PyTorch: \", y_pred_torch)\n",
        "assert(np.allclose(y_pred, y_pred_torch.detach().numpy().T))\n",
        "\n",
        "# lets compute the loss using PyTorch\n",
        "criterion = nn.MSELoss()\n",
        "loss = criterion(y_pred_torch, y_torch)\n",
        "print(\"\\nLoss using PyTorch: \", loss)\n",
        "assert(loss == computeLoss(y, y_pred))\n",
        "\n",
        "optimizer = optim.SGD(nn_torch.parameters(), lr=0.1)\n",
        "# lets compute the gradients using PyTorch\n",
        "optimizer.zero_grad()\n",
        "loss.backward()\n",
        "\n",
        "print(\"\\nWeight Gradient using PyTorch: \", nn_torch.weight.grad)\n",
        "assert(np.allclose(w0_grad, nn_torch.weight.grad.detach().numpy()))\n",
        "\n",
        "print(\"\\nBias Gradient using PyTorch: \", nn_torch.bias.grad)\n",
        "assert(np.allclose(b0_grad, nn_torch.bias.grad.detach().numpy()))\n",
        "\n",
        "# lets update the weights using pytorch\n",
        "optimizer.step()\n",
        "print(\"\\nUpdated Weights using PyTorch: \", nn_torch.weight.data)\n",
        "print(\"\\nUpdated Bias using PyTorch: \", nn_torch.bias.data)\n",
        "assert(np.allclose(nn_custom.layers[0].weights, nn_torch.weight.data.detach().numpy()))\n",
        "assert(np.allclose(nn_custom.layers[0].bias, nn_torch.bias.data.detach().numpy()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "# lets write a simple training loop to train the network using above weight and bias update rules\n",
        "def fit(nn: NeuralNetworkForwardPass, x_input: np.ndarray, y: np.ndarray, learning_rate: float, epochs: int) -> None:\n",
        "    \"\"\"\n",
        "    This function trains the neural network using the given inputs and outputs.\n",
        "    Args:\n",
        "        nn: NeuralNetworkForwardPass object - The neural network to be trained\n",
        "        x_input: np.ndarray - The input data\n",
        "        y: np.ndarray - The target output data\n",
        "        learning_rate: float - The learning rate to be used for training\n",
        "        epochs: int - The number of epochs to train the network\n",
        "    \"\"\"\n",
        "\n",
        "    loss_arr = np.zeros(epochs)\n",
        "    for epoch in range(epochs):\n",
        "        y_pred = nn.forward(x_input)\n",
        "        loss = computeLoss(y, y_pred)\n",
        "        loss_arr[epoch] = loss\n",
        "        print(\"Epoch: \", epoch, \" Loss: \", loss)\n",
        "\n",
        "        # Compute the gradients\n",
        "        weight_gradient = computeWeightGradient(y, y_pred, x_input)\n",
        "        bias_gradient = computeBiasGradient(y, y_pred)\n",
        "\n",
        "        # Update the weights and bias\n",
        "        nn.layers[0].weights = nn.layers[0].weights - learning_rate * weight_gradient\n",
        "        nn.layers[0].bias = nn.layers[0].bias - learning_rate * bias_gradient\n",
        "    return loss_arr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch:  0  Loss:  0.4761599999999997\n",
            "Epoch:  1  Loss:  0.14117222399999999\n",
            "Epoch:  2  Loss:  0.11406735605759999\n",
            "Epoch:  3  Loss:  0.10740550233882624\n",
            "Epoch:  4  Loss:  0.10319139855922732\n",
            "Epoch:  5  Loss:  0.10003653505798826\n",
            "Epoch:  6  Loss:  0.09763858854051023\n",
            "Epoch:  7  Loss:  0.09581371063605011\n",
            "Epoch:  8  Loss:  0.09442480964014158\n",
            "Epoch:  9  Loss:  0.09336771896461687\n",
            "Epoch:  10  Loss:  0.09256316815793046\n",
            "Epoch:  11  Loss:  0.09195082519706886\n",
            "Epoch:  12  Loss:  0.09148477146849301\n",
            "Epoch:  13  Loss:  0.09113005835529422\n",
            "Epoch:  14  Loss:  0.09086008649375994\n",
            "Epoch:  15  Loss:  0.09065461112984356\n",
            "Epoch:  16  Loss:  0.09049822399773047\n",
            "Epoch:  17  Loss:  0.09037919787885958\n",
            "Epoch:  18  Loss:  0.090288607196736\n",
            "Epoch:  19  Loss:  0.09021965870235962\n",
            "Epoch:  20  Loss:  0.09016718205944962\n",
            "Epoch:  21  Loss:  0.0901272421292741\n",
            "Epoch:  22  Loss:  0.09009684388094932\n",
            "Epoch:  23  Loss:  0.09007370779890933\n",
            "Epoch:  24  Loss:  0.09005609894571345\n",
            "Epoch:  25  Loss:  0.09004269686188883\n",
            "Epoch:  26  Loss:  0.09003249654680619\n",
            "Epoch:  27  Loss:  0.09002473309530509\n",
            "Epoch:  28  Loss:  0.09001882433869113\n",
            "Epoch:  29  Loss:  0.09001432718884503\n",
            "Epoch:  30  Loss:  0.09001090441176018\n",
            "Epoch:  31  Loss:  0.09000829933890882\n",
            "Epoch:  32  Loss:  0.09000631662008352\n",
            "Epoch:  33  Loss:  0.09000480757440057\n",
            "Epoch:  34  Loss:  0.09000365904096039\n",
            "Epoch:  35  Loss:  0.0900027848930946\n",
            "Epoch:  36  Loss:  0.09000211957986595\n",
            "Epoch:  37  Loss:  0.09000161321050953\n",
            "Epoch:  38  Loss:  0.09000122781320483\n",
            "Epoch:  39  Loss:  0.0900009344876301\n",
            "Epoch:  40  Loss:  0.09000071123777412\n",
            "Epoch:  41  Loss:  0.09000054132249055\n",
            "Epoch:  42  Loss:  0.09000041200010665\n",
            "Epoch:  43  Loss:  0.0900003135729456\n",
            "Epoch:  44  Loss:  0.09000023866011347\n",
            "Epoch:  45  Loss:  0.09000018164401799\n",
            "Epoch:  46  Loss:  0.09000013824911414\n",
            "Epoch:  47  Loss:  0.09000010522128814\n",
            "Epoch:  48  Loss:  0.09000008008383671\n",
            "Epoch:  49  Loss:  0.09000006095174289\n",
            "Epoch:  50  Loss:  0.09000004639032186\n",
            "Epoch:  51  Loss:  0.0900000353076362\n",
            "Epoch:  52  Loss:  0.09000002687261315\n",
            "Epoch:  53  Loss:  0.09000002045272396\n",
            "Epoch:  54  Loss:  0.09000001556655159\n",
            "Epoch:  55  Loss:  0.09000001184768971\n",
            "Epoch:  56  Loss:  0.09000000901726699\n",
            "Epoch:  57  Loss:  0.09000000686303454\n",
            "Epoch:  58  Loss:  0.09000000522345002\n",
            "Epoch:  59  Loss:  0.09000000397556354\n",
            "Epoch:  60  Loss:  0.09000000302579816\n",
            "Epoch:  61  Loss:  0.09000000230293251\n",
            "Epoch:  62  Loss:  0.09000000175276006\n",
            "Epoch:  63  Loss:  0.09000000133402426\n",
            "Epoch:  64  Loss:  0.09000000101532477\n",
            "Epoch:  65  Loss:  0.09000000077276285\n",
            "Epoch:  66  Loss:  0.09000000058814919\n",
            "Epoch:  67  Loss:  0.09000000044763987\n",
            "Epoch:  68  Loss:  0.09000000034069834\n",
            "Epoch:  69  Loss:  0.09000000025930523\n",
            "Epoch:  70  Loss:  0.09000000019735702\n",
            "Epoch:  71  Loss:  0.09000000015020825\n",
            "Epoch:  72  Loss:  0.09000000011432338\n",
            "Epoch:  73  Loss:  0.09000000008701144\n",
            "Epoch:  74  Loss:  0.09000000006622431\n",
            "Epoch:  75  Loss:  0.09000000005040329\n",
            "Epoch:  76  Loss:  0.0900000000383619\n",
            "Epoch:  77  Loss:  0.0900000000291972\n",
            "Epoch:  78  Loss:  0.09000000002222198\n",
            "Epoch:  79  Loss:  0.0900000000169131\n",
            "Epoch:  80  Loss:  0.09000000001287257\n",
            "Epoch:  81  Loss:  0.09000000000979731\n",
            "Epoch:  82  Loss:  0.09000000000745671\n",
            "Epoch:  83  Loss:  0.09000000000567529\n",
            "Epoch:  84  Loss:  0.09000000000431947\n",
            "Epoch:  85  Loss:  0.09000000000328753\n",
            "Epoch:  86  Loss:  0.09000000000250215\n",
            "Epoch:  87  Loss:  0.09000000000190436\n",
            "Epoch:  88  Loss:  0.09000000000144945\n",
            "Epoch:  89  Loss:  0.09000000000110317\n",
            "Epoch:  90  Loss:  0.09000000000083962\n",
            "Epoch:  91  Loss:  0.090000000000639\n",
            "Epoch:  92  Loss:  0.09000000000048636\n",
            "Epoch:  93  Loss:  0.09000000000037017\n",
            "Epoch:  94  Loss:  0.09000000000028173\n",
            "Epoch:  95  Loss:  0.09000000000021441\n",
            "Epoch:  96  Loss:  0.0900000000001632\n",
            "Epoch:  97  Loss:  0.09000000000012422\n",
            "Epoch:  98  Loss:  0.09000000000009455\n",
            "Epoch:  99  Loss:  0.09000000000007194\n"
          ]
        }
      ],
      "source": [
        "loss_arr = fit(nn_custom, x_input, y, 0.1, 100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x15f40b310>]"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# run cell 7 onwards multiple times to be able to see training loss variations for different inputs and outputs\n",
        "# training loss follows a U-curve and usually we select the weights and bias corresponding to least loss\n",
        "plt.plot(loss_arr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch:  0  Loss:  tensor(0.4762, grad_fn=<MseLossBackward0>)\n",
            "Epoch:  1  Loss:  tensor(0.1412, grad_fn=<MseLossBackward0>)\n",
            "Epoch:  2  Loss:  tensor(0.1141, grad_fn=<MseLossBackward0>)\n",
            "Epoch:  3  Loss:  tensor(0.1074, grad_fn=<MseLossBackward0>)\n",
            "Epoch:  4  Loss:  tensor(0.1032, grad_fn=<MseLossBackward0>)\n",
            "Epoch:  5  Loss:  tensor(0.1000, grad_fn=<MseLossBackward0>)\n",
            "Epoch:  6  Loss:  tensor(0.0976, grad_fn=<MseLossBackward0>)\n",
            "Epoch:  7  Loss:  tensor(0.0958, grad_fn=<MseLossBackward0>)\n",
            "Epoch:  8  Loss:  tensor(0.0944, grad_fn=<MseLossBackward0>)\n",
            "Epoch:  9  Loss:  tensor(0.0934, grad_fn=<MseLossBackward0>)\n",
            "Epoch:  10  Loss:  tensor(0.0926, grad_fn=<MseLossBackward0>)\n",
            "Epoch:  11  Loss:  tensor(0.0920, grad_fn=<MseLossBackward0>)\n",
            "Epoch:  12  Loss:  tensor(0.0915, grad_fn=<MseLossBackward0>)\n",
            "Epoch:  13  Loss:  tensor(0.0911, grad_fn=<MseLossBackward0>)\n",
            "Epoch:  14  Loss:  tensor(0.0909, grad_fn=<MseLossBackward0>)\n",
            "Epoch:  15  Loss:  tensor(0.0907, grad_fn=<MseLossBackward0>)\n",
            "Epoch:  16  Loss:  tensor(0.0905, grad_fn=<MseLossBackward0>)\n",
            "Epoch:  17  Loss:  tensor(0.0904, grad_fn=<MseLossBackward0>)\n",
            "Epoch:  18  Loss:  tensor(0.0903, grad_fn=<MseLossBackward0>)\n",
            "Epoch:  19  Loss:  tensor(0.0902, grad_fn=<MseLossBackward0>)\n",
            "Epoch:  20  Loss:  tensor(0.0902, grad_fn=<MseLossBackward0>)\n",
            "Epoch:  21  Loss:  tensor(0.0901, grad_fn=<MseLossBackward0>)\n",
            "Epoch:  22  Loss:  tensor(0.0901, grad_fn=<MseLossBackward0>)\n",
            "Epoch:  23  Loss:  tensor(0.0901, grad_fn=<MseLossBackward0>)\n",
            "Epoch:  24  Loss:  tensor(0.0901, grad_fn=<MseLossBackward0>)\n",
            "Epoch:  25  Loss:  tensor(0.0900, grad_fn=<MseLossBackward0>)\n",
            "Epoch:  26  Loss:  tensor(0.0900, grad_fn=<MseLossBackward0>)\n",
            "Epoch:  27  Loss:  tensor(0.0900, grad_fn=<MseLossBackward0>)\n",
            "Epoch:  28  Loss:  tensor(0.0900, grad_fn=<MseLossBackward0>)\n",
            "Epoch:  29  Loss:  tensor(0.0900, grad_fn=<MseLossBackward0>)\n",
            "Epoch:  30  Loss:  tensor(0.0900, grad_fn=<MseLossBackward0>)\n",
            "Epoch:  31  Loss:  tensor(0.0900, grad_fn=<MseLossBackward0>)\n",
            "Epoch:  32  Loss:  tensor(0.0900, grad_fn=<MseLossBackward0>)\n",
            "Epoch:  33  Loss:  tensor(0.0900, grad_fn=<MseLossBackward0>)\n",
            "Epoch:  34  Loss:  tensor(0.0900, grad_fn=<MseLossBackward0>)\n",
            "Epoch:  35  Loss:  tensor(0.0900, grad_fn=<MseLossBackward0>)\n",
            "Epoch:  36  Loss:  tensor(0.0900, grad_fn=<MseLossBackward0>)\n",
            "Epoch:  37  Loss:  tensor(0.0900, grad_fn=<MseLossBackward0>)\n",
            "Epoch:  38  Loss:  tensor(0.0900, grad_fn=<MseLossBackward0>)\n",
            "Epoch:  39  Loss:  tensor(0.0900, grad_fn=<MseLossBackward0>)\n",
            "Epoch:  40  Loss:  tensor(0.0900, grad_fn=<MseLossBackward0>)\n",
            "Epoch:  41  Loss:  tensor(0.0900, grad_fn=<MseLossBackward0>)\n",
            "Epoch:  42  Loss:  tensor(0.0900, grad_fn=<MseLossBackward0>)\n",
            "Epoch:  43  Loss:  tensor(0.0900, grad_fn=<MseLossBackward0>)\n",
            "Epoch:  44  Loss:  tensor(0.0900, grad_fn=<MseLossBackward0>)\n",
            "Epoch:  45  Loss:  tensor(0.0900, grad_fn=<MseLossBackward0>)\n",
            "Epoch:  46  Loss:  tensor(0.0900, grad_fn=<MseLossBackward0>)\n",
            "Epoch:  47  Loss:  tensor(0.0900, grad_fn=<MseLossBackward0>)\n",
            "Epoch:  48  Loss:  tensor(0.0900, grad_fn=<MseLossBackward0>)\n",
            "Epoch:  49  Loss:  tensor(0.0900, grad_fn=<MseLossBackward0>)\n",
            "Epoch:  50  Loss:  tensor(0.0900, grad_fn=<MseLossBackward0>)\n",
            "Epoch:  51  Loss:  tensor(0.0900, grad_fn=<MseLossBackward0>)\n",
            "Epoch:  52  Loss:  tensor(0.0900, grad_fn=<MseLossBackward0>)\n",
            "Epoch:  53  Loss:  tensor(0.0900, grad_fn=<MseLossBackward0>)\n",
            "Epoch:  54  Loss:  tensor(0.0900, grad_fn=<MseLossBackward0>)\n",
            "Epoch:  55  Loss:  tensor(0.0900, grad_fn=<MseLossBackward0>)\n",
            "Epoch:  56  Loss:  tensor(0.0900, grad_fn=<MseLossBackward0>)\n",
            "Epoch:  57  Loss:  tensor(0.0900, grad_fn=<MseLossBackward0>)\n",
            "Epoch:  58  Loss:  tensor(0.0900, grad_fn=<MseLossBackward0>)\n",
            "Epoch:  59  Loss:  tensor(0.0900, grad_fn=<MseLossBackward0>)\n",
            "Epoch:  60  Loss:  tensor(0.0900, grad_fn=<MseLossBackward0>)\n",
            "Epoch:  61  Loss:  tensor(0.0900, grad_fn=<MseLossBackward0>)\n",
            "Epoch:  62  Loss:  tensor(0.0900, grad_fn=<MseLossBackward0>)\n",
            "Epoch:  63  Loss:  tensor(0.0900, grad_fn=<MseLossBackward0>)\n",
            "Epoch:  64  Loss:  tensor(0.0900, grad_fn=<MseLossBackward0>)\n",
            "Epoch:  65  Loss:  tensor(0.0900, grad_fn=<MseLossBackward0>)\n",
            "Epoch:  66  Loss:  tensor(0.0900, grad_fn=<MseLossBackward0>)\n",
            "Epoch:  67  Loss:  tensor(0.0900, grad_fn=<MseLossBackward0>)\n",
            "Epoch:  68  Loss:  tensor(0.0900, grad_fn=<MseLossBackward0>)\n",
            "Epoch:  69  Loss:  tensor(0.0900, grad_fn=<MseLossBackward0>)\n",
            "Epoch:  70  Loss:  tensor(0.0900, grad_fn=<MseLossBackward0>)\n",
            "Epoch:  71  Loss:  tensor(0.0900, grad_fn=<MseLossBackward0>)\n",
            "Epoch:  72  Loss:  tensor(0.0900, grad_fn=<MseLossBackward0>)\n",
            "Epoch:  73  Loss:  tensor(0.0900, grad_fn=<MseLossBackward0>)\n",
            "Epoch:  74  Loss:  tensor(0.0900, grad_fn=<MseLossBackward0>)\n",
            "Epoch:  75  Loss:  tensor(0.0900, grad_fn=<MseLossBackward0>)\n",
            "Epoch:  76  Loss:  tensor(0.0900, grad_fn=<MseLossBackward0>)\n",
            "Epoch:  77  Loss:  tensor(0.0900, grad_fn=<MseLossBackward0>)\n",
            "Epoch:  78  Loss:  tensor(0.0900, grad_fn=<MseLossBackward0>)\n",
            "Epoch:  79  Loss:  tensor(0.0900, grad_fn=<MseLossBackward0>)\n",
            "Epoch:  80  Loss:  tensor(0.0900, grad_fn=<MseLossBackward0>)\n",
            "Epoch:  81  Loss:  tensor(0.0900, grad_fn=<MseLossBackward0>)\n",
            "Epoch:  82  Loss:  tensor(0.0900, grad_fn=<MseLossBackward0>)\n",
            "Epoch:  83  Loss:  tensor(0.0900, grad_fn=<MseLossBackward0>)\n",
            "Epoch:  84  Loss:  tensor(0.0900, grad_fn=<MseLossBackward0>)\n",
            "Epoch:  85  Loss:  tensor(0.0900, grad_fn=<MseLossBackward0>)\n",
            "Epoch:  86  Loss:  tensor(0.0900, grad_fn=<MseLossBackward0>)\n",
            "Epoch:  87  Loss:  tensor(0.0900, grad_fn=<MseLossBackward0>)\n",
            "Epoch:  88  Loss:  tensor(0.0900, grad_fn=<MseLossBackward0>)\n",
            "Epoch:  89  Loss:  tensor(0.0900, grad_fn=<MseLossBackward0>)\n",
            "Epoch:  90  Loss:  tensor(0.0900, grad_fn=<MseLossBackward0>)\n",
            "Epoch:  91  Loss:  tensor(0.0900, grad_fn=<MseLossBackward0>)\n",
            "Epoch:  92  Loss:  tensor(0.0900, grad_fn=<MseLossBackward0>)\n",
            "Epoch:  93  Loss:  tensor(0.0900, grad_fn=<MseLossBackward0>)\n",
            "Epoch:  94  Loss:  tensor(0.0900, grad_fn=<MseLossBackward0>)\n",
            "Epoch:  95  Loss:  tensor(0.0900, grad_fn=<MseLossBackward0>)\n",
            "Epoch:  96  Loss:  tensor(0.0900, grad_fn=<MseLossBackward0>)\n",
            "Epoch:  97  Loss:  tensor(0.0900, grad_fn=<MseLossBackward0>)\n",
            "Epoch:  98  Loss:  tensor(0.0900, grad_fn=<MseLossBackward0>)\n",
            "Epoch:  99  Loss:  tensor(0.0900, grad_fn=<MseLossBackward0>)\n"
          ]
        }
      ],
      "source": [
        "# validate the training using PyTorch\n",
        "loss_torch = np.zeros(100)\n",
        "for epoch in range(100):\n",
        "    optimizer.zero_grad()\n",
        "    y_pred_torch = nn_torch(x_input_torch)\n",
        "    loss = criterion(y_pred_torch, y_torch)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    print(\"Epoch: \", epoch, \" Loss: \", loss)\n",
        "    loss_torch[epoch] = loss\n",
        "\n",
        "assert(loss_arr[-1].round(1) == loss_torch[-1].round(1))\n",
        "assert(np.allclose(nn_custom.layers[0].weights, nn_torch.weight.data.detach().numpy()))\n",
        "assert(np.allclose(nn_custom.layers[0].bias, nn_torch.bias.data.detach().numpy()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Multi-dimensional Input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Input: \n",
            " [[3 2 4 1 2]\n",
            " [1 1 2 2 4]]\n",
            "\n",
            "Target output:  [[1 0 0 0 0]]\n",
            "\n",
            "Weights:  [[1]\n",
            " [0]]\n",
            "\n",
            "Bias:  [[0]]\n",
            "\n",
            "Predicted output:  [[3 2 4 1 2]]\n",
            "\n",
            "Loss:  5.8\n",
            "\n",
            "Weight Gradient:  [[12.4]\n",
            " [ 8.8]]\n",
            "\n",
            "Bias Gradient:  4.4\n",
            "\n",
            "Updated Weights:  [[-0.24]\n",
            " [-0.88]]\n",
            "\n",
            "Updated Bias:  [[-0.44]]\n",
            "Epoch:  0  Loss:  9.626880000000005\n",
            "Epoch:  1  Loss:  16.436204544000013\n",
            "Epoch:  2  Loss:  28.353555010355212\n",
            "Epoch:  3  Loss:  49.10605875165269\n",
            "Epoch:  4  Loss:  85.18771639976873\n",
            "Epoch:  5  Loss:  147.8913444904052\n",
            "Epoch:  6  Loss:  256.8428624041216\n",
            "Epoch:  7  Loss:  446.14401094684763\n",
            "Epoch:  8  Loss:  775.0460963353449\n",
            "Epoch:  9  Loss:  1346.4956010401281\n",
            "\n",
            "Final Weights:  [[-10.23828678]\n",
            " [ -8.85523052]]\n",
            "\n",
            "Final Bias:  [[-4.13774005]]\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# lets increase the complexity of the network by having multi-dimensional inputs\n",
        "# first test if our existing code works for multi-dimensional inputs\n",
        "\n",
        "# lets generate some sample inputs and outputs\n",
        "# We'll start with 2-D input and 1-D output & a simpler network of just 1 layer mapping input to output\n",
        "input_dimensions = 2\n",
        "output_dimensions = 1 \n",
        "input_generator = SampleInputOutputUtils(input_dimensions, output_dimensions)\n",
        "x_input, y = input_generator.getSampleInputOutputBatch(5)\n",
        "\n",
        "print(\"\\nInput: \\n\", x_input)\n",
        "print(\"\\nTarget output: \", y)\n",
        "\n",
        "# lets initialize our basic neural network with just 1 layer mapping inputs to outputs\n",
        "nn_custom = NeuralNetworkForwardPass(input_dimensions, output_dimensions, ActivationFunctions.linear, [])\n",
        "\n",
        "# For debugging purposes, lets print the weights and bias of the network\n",
        "print(\"\\nWeights: \", nn_custom.layers[0].weights)\n",
        "print(\"\\nBias: \", nn_custom.layers[0].bias)\n",
        "\n",
        "y_pred = nn_custom.forward(x_input)\n",
        "\n",
        "print(\"\\nPredicted output: \", y_pred)\n",
        "print(\"\\nLoss: \", computeLoss(y, y_pred))\n",
        "\n",
        "print(\"\\nWeight Gradient: \", computeWeightGradient(y, y_pred, x_input))\n",
        "print(\"\\nBias Gradient: \", computeBiasGradient(y, y_pred))\n",
        "\n",
        "w0, b0 = nn_custom.layers[0].weights, nn_custom.layers[0].bias\n",
        "w0_grad, b0_grad = computeWeightGradient(y, y_pred, x_input), computeBiasGradient(y, y_pred)\n",
        "# lets update the weights using the gradients\n",
        "learning_rate = 0.1\n",
        "nn_custom.layers[0].weights = nn_custom.layers[0].weights - learning_rate * w0_grad\n",
        "print(\"\\nUpdated Weights: \", nn_custom.layers[0].weights)\n",
        "\n",
        "# lets update the bias using the gradients\n",
        "nn_custom.layers[0].bias = nn_custom.layers[0].bias - learning_rate * b0_grad\n",
        "print(\"\\nUpdated Bias: \", nn_custom.layers[0].bias)\n",
        "\n",
        "loss_arr = fit(nn_custom, x_input, y, 0.1, 10)\n",
        "plt.plot(loss_arr)\n",
        "\n",
        "print(\"\\nFinal Weights: \", nn_custom.layers[0].weights)\n",
        "print(\"\\nFinal Bias: \", nn_custom.layers[0].bias)\n",
        "\n",
        "# so we were able to train the network with multi-dimensional inputs with our existing code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(array([[1],\n",
              "        [0]]),\n",
              " array([[0]]))"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "w0, b0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch:  0  Loss:  tensor(5.8000, grad_fn=<MseLossBackward0>)\n",
            "Epoch:  1  Loss:  tensor(9.6269, grad_fn=<MseLossBackward0>)\n",
            "Epoch:  2  Loss:  tensor(16.4362, grad_fn=<MseLossBackward0>)\n",
            "Epoch:  3  Loss:  tensor(28.3536, grad_fn=<MseLossBackward0>)\n",
            "Epoch:  4  Loss:  tensor(49.1061, grad_fn=<MseLossBackward0>)\n",
            "Epoch:  5  Loss:  tensor(85.1878, grad_fn=<MseLossBackward0>)\n",
            "Epoch:  6  Loss:  tensor(147.8914, grad_fn=<MseLossBackward0>)\n",
            "Epoch:  7  Loss:  tensor(256.8430, grad_fn=<MseLossBackward0>)\n",
            "Epoch:  8  Loss:  tensor(446.1444, grad_fn=<MseLossBackward0>)\n",
            "Epoch:  9  Loss:  tensor(775.0470, grad_fn=<MseLossBackward0>)\n",
            "Epoch:  10  Loss:  tensor(1346.4973, grad_fn=<MseLossBackward0>)\n",
            "\n",
            "Final Weights using PyTorch:  tensor([[-10.2383,  -8.8552]])\n",
            "\n",
            "Final Bias using PyTorch:  tensor([[-4.1377]])\n"
          ]
        }
      ],
      "source": [
        "# lets validate the training using PyTorch\n",
        "# lets convert the numpy arrays to torch tensors\n",
        "\n",
        "x_input_torch = torch.tensor(x_input.T, dtype=torch.float32)\n",
        "y_torch = torch.tensor(y.T, dtype=torch.float32)\n",
        "\n",
        "# lets initialize our basic neural network with just 1 layer mapping inputs to outputs\n",
        "nn_torch = nn.Linear(input_dimensions, output_dimensions, bias=True)\n",
        "nn_torch.weight.data = torch.tensor(w0.T, dtype=torch.float32) # we need to transpose the weights because of the way PyTorch initializes weights\n",
        "nn_torch.bias.data = torch.tensor(b0.T, dtype=torch.float32) # we need to transpose the bias because of the way PyTorch initializes bias\n",
        "\n",
        "optimizer = optim.SGD(nn_torch.parameters(), lr=0.1)\n",
        "loss_torch = np.zeros(11)\n",
        "for epoch in range(11): # we are running for 11 epochs to match the number of epochs in our custom code 10 in fit step + 1 during step by step run\n",
        "    y_pred_torch = nn_torch(x_input_torch)\n",
        "    loss = criterion(y_pred_torch, y_torch)\n",
        "    print(\"Epoch: \", epoch, \" Loss: \", loss)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    loss_torch[epoch] = loss\n",
        "\n",
        "assert(loss_arr[-1].round(1) == loss_torch[-1].round(1))\n",
        "assert(np.allclose(nn_custom.layers[0].weights, nn_torch.weight.data.detach().numpy().T))\n",
        "assert(np.allclose(nn_custom.layers[0].bias, nn_torch.bias.data.detach().numpy().T))\n",
        "print(\"\\nFinal Weights using PyTorch: \", nn_torch.weight.data)\n",
        "print(\"\\nFinal Bias using PyTorch: \", nn_torch.bias.data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Training with multiple layers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Input: \n",
            " [[3 3 4 3 0]\n",
            " [2 1 0 3 1]]\n",
            "\n",
            "Target output:  [[1 1 0 0 0]]\n",
            "\n",
            "Hidden Layer Weights:  [[ 0  0]\n",
            " [ 0 -1]]\n",
            "\n",
            "Hidden Layer Bias:  [[0]\n",
            " [0]]\n",
            "\n",
            "Output Layer Weights:  [[ 1]\n",
            " [-1]]\n",
            "\n",
            "Output Layer Bias:  [[0]]\n",
            "\n",
            "Predicted output:  [[2 1 0 3 1]]\n",
            "\n",
            "Loss:  2.2\n",
            "\n",
            "Output layer Weight Gradient:  [[-0. ]\n",
            " [-4.8]]\n",
            "\n",
            "Output layer Bias Gradient:  2.0\n"
          ]
        }
      ],
      "source": [
        "# lets increase the complexity of the network by having multi-dimensional inputs and multi-layer network\n",
        "# first test if our existing code works for multi-layer network\n",
        "\n",
        "# lets generate some sample inputs and outputs  - 2-D input and 1-D output & a network with 2 layers\n",
        "input_dimensions = 2\n",
        "output_dimensions = 1\n",
        "hidden_layer_neurons = 2\n",
        "input_generator = SampleInputOutputUtils(input_dimensions, output_dimensions)\n",
        "x_input, y = input_generator.getSampleInputOutputBatch(5)\n",
        "\n",
        "print(\"\\nInput: \\n\", x_input)\n",
        "print(\"\\nTarget output: \", y)\n",
        "\n",
        "# lets initialize our basic neural network with 2 layers - 1 hidden layer and 1 output layer\n",
        "nn_custom = NeuralNetworkForwardPass(input_dimensions, output_dimensions, ActivationFunctions.linear, [(hidden_layer_neurons, ActivationFunctions.linear)])\n",
        "\n",
        "# For debugging purposes, lets print the weights and bias of the network\n",
        "print(\"\\nHidden Layer Weights: \", nn_custom.layers[0].weights)\n",
        "print(\"\\nHidden Layer Bias: \", nn_custom.layers[0].bias)\n",
        "\n",
        "print(\"\\nOutput Layer Weights: \", nn_custom.layers[1].weights)\n",
        "print(\"\\nOutput Layer Bias: \", nn_custom.layers[1].bias)\n",
        "\n",
        "y_pred = nn_custom.forward(x_input)\n",
        "\n",
        "print(\"\\nPredicted output: \", y_pred)\n",
        "print(\"\\nLoss: \", computeLoss(y, y_pred))\n",
        "\n",
        "# since input to output layer is hidden layer's output\n",
        "print(\"\\nOutput layer Weight Gradient: \", computeWeightGradient(y, y_pred, nn_custom.layers[0].forward(x_input)))\n",
        "print(\"\\nOutput layer Bias Gradient: \", computeBiasGradient(y, y_pred))\n",
        "\n",
        "# so computation of gradient for final layer is going to be same as before and we will be able to update the weights and bias of the final layer\n",
        "# however for hidden layer, we can't use the same method because its output is input to another layer which then results in final output\n",
        "# so lets go back to drawing board and analyze how we can compute the gradients for hidden layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Hidden Layer gradient computation\n",
        "Lets represent hidden layer weights, bias & output as ${hw}$, ${hb}$, and ${h\\hat y}$. And calculate relationship between ${hw}$ and ${L}$. We know that\n",
        "$$\n",
        "L = (y - \\hat y)^2\n",
        "$$\n",
        "\n",
        "And ${\\hat y}$ = ${w \\cdot h\\hat y + b}$ since hidden layers output is input to the output layer. And hidden layer output is a function of hidden layer weights and bias ${h\\hat y}$ = ${hw \\cdot x + hb}$. \n",
        "\n",
        "So we can see that this relationship takes form of ${f(g(x))}$ where we can think of ${g(x)}$ as output of hidden layer and ${f}$ as output of final layer. And by chain rule we can differentiate this as follows: \n",
        "$$\n",
        "\\frac{\\partial f(g(x))}{\\partial x} = \\frac{\\partial f(g(x))}{\\partial g(x)} \\cdot \\frac{\\partial g(x)}{\\partial x}\n",
        "$$\n",
        "\n",
        "Now we are going to calculate partial derivative of loss function wrt ${hw}$ -\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial hw} = \\frac{\\partial}{\\partial hw} (y - (w \\cdot h\\hat y + b))^2\n",
        "$$\n",
        "\n",
        "Applying the chain rule:\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial hw} = 2 (y -(w \\cdot h\\hat y + b)) \\cdot \\frac{\\partial}{\\partial h\\hat y} (y - (w \\cdot h\\hat y + b)) \\cdot \\frac{\\partial}{\\partial hw} (h\\hat y) \n",
        "$$\n",
        "\n",
        "Differentiating the first inner term (since $y$ is a constant and $\\frac{\\partial b}{\\partial h\\hat y}$ = 0):\n",
        "$$\n",
        "\\frac{\\partial}{\\partial h\\hat y} (y - (w \\cdot h\\hat y + b)) = -w\n",
        "$$\n",
        "\n",
        "Differentiating the second inner term (since $y$ is a constant and $\\frac{\\partial b}{\\partial h\\hat y}$ = 0):\n",
        "$$\n",
        "\\frac{\\partial}{\\partial hw} (h\\hat y) = \\frac{\\partial}{\\partial hw} (hw.x + hb) = x\n",
        "$$\n",
        "\n",
        "Combining the results:\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial hw} = 2 (y - (w \\cdot h\\hat y + b)) \\cdot (-w) \\cdot (x) = -2 w \\cdot x (y - (w \\cdot h\\hat y + b)) = -2 w \\cdot x (y - \\hat{y})\n",
        "$$\n",
        "\n",
        "Now, we can extend above equation to multiple record dataset by taking average of these individual sample derivatives:\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial hw} = -\\frac{2 \\cdot w}{n} \\cdot \\sum_{i=1}^{n} x_i(y_i - \\hat{y}_i)\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "# lets write down the function to get the gradient of the loss function with respect to the hidden layer weights\n",
        "\n",
        "def computeHiddenLayerWeightGradient(nn, y, y_pred, x_input):\n",
        "    \"\"\"\n",
        "    This function computes the gradient of the loss function with respect to the hidden layer weights.\n",
        "    Args:\n",
        "        nn: NeuralNetworkForwardPass object - The neural network to be trained\n",
        "        y: np.ndarray - The target output data\n",
        "        y_pred: np.ndarray - The predicted output data\n",
        "        x_input: np.ndarray - The input data\n",
        "    \"\"\"\n",
        "    if y is None or y_pred is None or x_input is None or y.size == 0 or y_pred.size == 0 or x_input.size == 0:\n",
        "        raise ValueError(\"y and y_pred cannot be None/empty\")\n",
        "    \n",
        "    if y.size != y_pred.size:\n",
        "        raise ValueError(\"y, y_pred should be of same length\")\n",
        "    if y.size != x_input.shape[-1]:\n",
        "        raise ValueError(\"y and x_input should have same number of records\")\n",
        "    \n",
        "    axis_avg = (x_input.ndim - 1) if x_input.ndim > 1 else None #last axis represents batch of inputs and we want to average over the batch\n",
        "    \n",
        "    # we need to multiply the error in the output layer with the weights of the output layer [output layer weights are updated after hidden layer weights are updated]\n",
        "    # we want element wise multiplication of the two arrays here because for a batch of inputs, \n",
        "    # we are multiplying each weight * input with the corresponding error\n",
        "    # we have put a transpose here because we have transposed our weight matrix during initialization itself\n",
        "    return -2 * np.mean((y - y_pred) *  x_input, axis=axis_avg, keepdims=True) @ nn.layers[1].weights.T"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Similarly we can compute the gradient for hidden layer bias\n",
        "\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial hb} = -\\frac{2 \\cdot w}{n} \\cdot \\sum_{i=1}^{n} (y_i - \\hat{y}_i)\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "def computeHiddenLayerBiasGradient(nn, y, y_pred, x_input):\n",
        "    \"\"\"\n",
        "    This function computes the gradient of the loss function with respect to the hidden layer weights.\n",
        "    Args:\n",
        "        nn: NeuralNetworkForwardPass object - The neural network to be trained\n",
        "        y: np.ndarray - The target output data\n",
        "        y_pred: np.ndarray - The predicted output data\n",
        "        x_input: np.ndarray - The input data\n",
        "    \"\"\"\n",
        "    if y is None or y_pred is None or x_input is None or y.size == 0 or y_pred.size == 0 or x_input.size == 0:\n",
        "        raise ValueError(\"y and y_pred cannot be None/empty\")\n",
        "    \n",
        "    if y.size != y_pred.size:\n",
        "        raise ValueError(\"y, y_pred should be of same length\")\n",
        "    \n",
        "    return -2 * np.mean(y - y_pred) * np.reshape(nn.layers[1].weights, (nn.input_dimensions, 1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " Hidden Layer output:  [[ 0  0  0  0  0]\n",
            " [-2 -1  0 -3 -1]]\n",
            "\n",
            "Hidden Layer Weight gradient:  [[ 4.8 -4.8]\n",
            " [ 4.8 -4.8]]\n",
            "\n",
            "Hidden Layer bias gradient:  [[ 2.]\n",
            " [-2.]]\n",
            "\n",
            "Updated Hidden Layer Weights:  [[-0.48  0.48]\n",
            " [-0.48 -0.52]]\n",
            "\n",
            "Updated Hidden Layer Bias:  [[-0.2]\n",
            " [ 0.2]]\n",
            "\n",
            "Updated Weights:  [[ 1.  ]\n",
            " [-0.52]]\n",
            "\n",
            "Updated Bias:  [[-0.2]]\n"
          ]
        }
      ],
      "source": [
        "# lets update the weights using the gradients\n",
        "# we need to store hidden layer output for output layer weight update rule\n",
        "w0, b0 = nn_custom.layers[0].weights, nn_custom.layers[0].bias\n",
        "w1, b1 = nn_custom.layers[1].weights, nn_custom.layers[1].bias\n",
        "\n",
        "hidden_layer_output = nn_custom.layers[0].forward(x_input)\n",
        "print(\"\\n Hidden Layer output: \", hidden_layer_output)\n",
        "learning_rate = 0.1\n",
        "\n",
        "w0_grad, b0_grad = computeHiddenLayerWeightGradient(nn_custom, y, y_pred, x_input), computeHiddenLayerBiasGradient(nn_custom, y, y_pred, x_input)\n",
        "w1_grad, b1_grad = computeWeightGradient(y, y_pred, hidden_layer_output), computeBiasGradient(y, y_pred)\n",
        "\n",
        "print(\"\\nHidden Layer Weight gradient: \", computeHiddenLayerWeightGradient(nn_custom, y, y_pred, x_input))\n",
        "print(\"\\nHidden Layer bias gradient: \", computeHiddenLayerBiasGradient(nn_custom, y, y_pred, x_input))\n",
        "\n",
        "nn_custom.layers[0].weights = nn_custom.layers[0].weights - learning_rate * w0_grad\n",
        "nn_custom.layers[0].bias = nn_custom.layers[0].bias - learning_rate * b0_grad\n",
        "print(\"\\nUpdated Hidden Layer Weights: \", nn_custom.layers[0].weights)\n",
        "print(\"\\nUpdated Hidden Layer Bias: \", nn_custom.layers[0].bias)\n",
        "\n",
        "nn_custom.layers[1].weights = nn_custom.layers[1].weights - learning_rate * w1_grad\n",
        "print(\"\\nUpdated Weights: \", nn_custom.layers[1].weights)\n",
        "\n",
        "nn_custom.layers[1].bias = nn_custom.layers[1].bias - learning_rate * b1_grad\n",
        "print(\"\\nUpdated Bias: \", nn_custom.layers[1].bias)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Loss using PyTorch:  tensor(2.2000, grad_fn=<MseLossBackward0>)\n",
            "\n",
            "Hidden Layer Weight gradient using PyTorch:  tensor([[ 4.8000,  4.8000],\n",
            "        [-4.8000, -4.8000]])\n",
            "\n",
            "Hidden Layer Bias gradient using PyTorch:  tensor([[ 2., -2.]])\n",
            "\n",
            "Output Layer Weight gradient using PyTorch:  tensor([[ 0.0000, -4.8000]])\n",
            "\n",
            "Output Layer Bias gradient using PyTorch:  tensor([[2.]])\n",
            "\n",
            "Updated Hidden Layer Weights using PyTorch:  tensor([[-0.4800, -0.4800],\n",
            "        [ 0.4800, -0.5200]])\n",
            "\n",
            "Updated Hidden Layer Bias using PyTorch:  tensor([[-0.2000,  0.2000]])\n",
            "\n",
            "Updated Output Layer Weights using PyTorch:  tensor([[ 1.0000, -0.5200]])\n",
            "\n",
            "Updated Output Layer Bias using PyTorch:  tensor([[-0.2000]])\n"
          ]
        }
      ],
      "source": [
        "# lets validate the multi-layer network gradient and parameter updates using PyTorch\n",
        "# lets convert the numpy arrays to torch tensors\n",
        "x_input_torch = torch.tensor(x_input.T, dtype=torch.float32)\n",
        "y_torch = torch.tensor(y.T, dtype=torch.float32)\n",
        "\n",
        "# lets initialize our basic neural network with 2 layers - 1 hidden layer and 1 output layer\n",
        "nn_torch = nn.Sequential(\n",
        "    nn.Linear(input_dimensions, hidden_layer_neurons, bias=True), \n",
        "    nn.Linear(hidden_layer_neurons, output_dimensions, bias=True)\n",
        "    )\n",
        "\n",
        "nn_torch[0].weight.data = torch.tensor(w0.T, dtype=torch.float32)\n",
        "nn_torch[0].bias.data = torch.tensor(b0.T, dtype=torch.float32)\n",
        "nn_torch[1].weight.data = torch.tensor(w1.T, dtype=torch.float32)\n",
        "nn_torch[1].bias.data = torch.tensor(b1.T, dtype=torch.float32)\n",
        "\n",
        "optimizer = optim.SGD(nn_torch.parameters(), lr=0.1)\n",
        "y_pred_torch = nn_torch(x_input_torch)\n",
        "optimizer.zero_grad()\n",
        "loss = criterion(y_pred_torch, y_torch)\n",
        "print(\"\\nLoss using PyTorch: \", loss)\n",
        "loss.backward()\n",
        "optimizer.step()\n",
        "\n",
        "assert(np.allclose(nn_torch[0].weight.grad.detach().numpy().T, w0_grad))\n",
        "assert(np.allclose(nn_torch[0].bias.grad.detach().numpy().T, b0_grad))\n",
        "assert(np.allclose(nn_torch[1].weight.grad.detach().numpy().T, w1_grad))\n",
        "assert(np.allclose(nn_torch[1].bias.grad.detach().numpy().T, b1_grad))\n",
        "\n",
        "assert(np.allclose(nn_custom.layers[0].weights, nn_torch[0].weight.data.detach().numpy().T))\n",
        "assert(np.allclose(nn_custom.layers[0].bias, nn_torch[0].bias.data.detach().numpy().T))\n",
        "assert(np.allclose(nn_custom.layers[1].weights, nn_torch[1].weight.data.detach().numpy().T))\n",
        "assert(np.allclose(nn_custom.layers[1].bias, nn_torch[1].bias.data.detach().numpy().T))\n",
        "\n",
        "print(\"\\nHidden Layer Weight gradient using PyTorch: \", nn_torch[0].weight.grad)\n",
        "print(\"\\nHidden Layer Bias gradient using PyTorch: \", nn_torch[0].bias.grad)\n",
        "\n",
        "print(\"\\nOutput Layer Weight gradient using PyTorch: \", nn_torch[1].weight.grad)\n",
        "print(\"\\nOutput Layer Bias gradient using PyTorch: \", nn_torch[1].bias.grad)\n",
        "\n",
        "print(\"\\nUpdated Hidden Layer Weights using PyTorch: \", nn_torch[0].weight.data)\n",
        "print(\"\\nUpdated Hidden Layer Bias using PyTorch: \", nn_torch[0].bias.data)\n",
        "\n",
        "print(\"\\nUpdated Output Layer Weights using PyTorch: \", nn_torch[1].weight.data)\n",
        "print(\"\\nUpdated Output Layer Bias using PyTorch: \", nn_torch[1].bias.data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yxlt8YD7Xrd_"
      },
      "source": [
        "# Recap\n",
        "\n",
        "This wraps up this notebook. Just to recap we learned -\n",
        "\n",
        "\n",
        "1.   Error and Loss computation\n",
        "2.   Gradient calculation\n",
        "3.   Backpropagation\n",
        "4.   to code\n",
        "    \n",
        "        *   loss computation\n",
        "        *   gradient computation for a single layer network\n",
        "        *   fit method implementation for a single layer network\n",
        "        *   gradient computation for a double layer network [backpropagation in practice]\n",
        "        *   linear neural network using pytorch and run validation on our custom implementation and pytorch implementation\n",
        "\n",
        "In the next notebook, we will learn how to generalize the fit method for a multi-layer network. Stay tuned."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lVyZOIeimr34"
      },
      "source": [
        "# References"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5puTbYau9hpq"
      },
      "source": [
        "1.   https://aibyhand.substack.com/\n",
        "2.   https://en.wikipedia.org/wiki/Matrix_calculus"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
