{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LixRLTwzMBsZ"
      },
      "source": [
        "The idea behind this notebook series is to go slow and build a solid understanding of code and maths involved in deep learning. Few points to note -\n",
        "1. For simplicity of calculations, I have used small integer values in examples\n",
        "2. This code is only for understanding the concepts so it is missing couple of things like type checking and error handling\n",
        "\n",
        "This is the 1st notebook in the series. Here we start with a single neuron (scalar input and output) and build upto the idea of a network layer.\n",
        "\n",
        "[Colab link](https://colab.research.google.com/github/shwetaAgrawal/deeplearning_tutorials/blob/main/notebooks/1_Intro_to_perceptron.ipynb)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Pre-requisite for running this notebook - numpy. If you come across error \"No module named 'numpy'\" then please uncomment the below line and run this cell\n",
        "#!pip install numpy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "osEjQ7iSmFKp"
      },
      "source": [
        "# Introduction to perceptron"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hSb87NDa9tQ9"
      },
      "source": [
        "**Lets start from a simple perceptron**\n",
        "\n",
        "How does a perceptron work?\n",
        "It takes in an **input** - applies **weight** and **intercept** followed by an **activation** function\n",
        "\n",
        "> *y_pred = activation_function(w * x_input + b)*\n",
        "\n",
        "This notebook follows the concepts covered in [Linear Layer Worksheet by Tom Yeh](https://aibyhand.substack.com/p/w3-linear-layer). If you are like me, you might want to grab the pen and notebook to do the exercises covered in the linked workbook before going through code here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "3SDoNbxR9f04"
      },
      "outputs": [],
      "source": [
        "# numpy is used for efficient numerical operations - it supports vectorized operations on n-dimensional arrays\n",
        "# random is used to generate random numbers for generating input data\n",
        "import numpy as np\n",
        "import random"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4rTbuztX1MAx"
      },
      "source": [
        "## Scalar input\n",
        "Perceptron with simple scalar input, weight and bias.\n",
        "\n",
        "For perceptron scalar input is nothing but a 1x1 matrix. We will be covering this later in the notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X4wtY3riqnYE",
        "outputId": "f9ef256d-3692-427b-bcf0-1eef9006a143"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input, x = 4\n",
            "Prediction, y_pred = 13\n"
          ]
        }
      ],
      "source": [
        "# Perceptron example with scalar input. We are starting with barebones code to understand the concept\n",
        "x = random.randint(1, 10)\n",
        "print(\"Input, x =\", x)\n",
        "weight = 3\n",
        "bias = 1\n",
        "\n",
        "# lets say activation function is linear f(x) = x for all values of x\n",
        "y_pred = (weight * x + bias)\n",
        "print(\"Prediction, y_pred =\", y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input, x = 4\n",
            "Prediction, y_pred = 13\n"
          ]
        }
      ],
      "source": [
        "#Lets define the perceptron class \n",
        "class Perceptron:\n",
        "    \"\"\"\n",
        "    A simple perceptron implementation.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, weight, bias):\n",
        "        \"\"\"\n",
        "        Initialize the perceptron with the given weight and bias.\n",
        "        \n",
        "        Args:\n",
        "            weight (float/int) : The weight of the perceptron.\n",
        "            bias (float/int) : The bias of the perceptron.\n",
        "        \"\"\"\n",
        "        self.weight = weight\n",
        "        self.bias = bias\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Calculate the output of the perceptron for the given input.\n",
        "        \n",
        "        Args:\n",
        "            x (float/int/np.ndarray): The input to the perceptron.\n",
        "    \n",
        "        Returns:\n",
        "            float/int/np.ndarray: The output of the perceptron.\n",
        "        \"\"\"\n",
        "        return self.weight * x + self.bias\n",
        "    \n",
        "# Lets create an object of the perceptron class and test it\n",
        "perceptron1 = Perceptron(weight, bias)\n",
        "\n",
        "# Lets test using the same input x that we used in earlier cell\n",
        "print(\"Input, x =\", x)\n",
        "y_pred = perceptron1.forward(x)\n",
        "print(\"Prediction, y_pred =\", y_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HOo80Qo72BO4"
      },
      "source": [
        "## Activation Functions\n",
        "\n",
        "Lets define some common activation functions used in deep learning. Activation function are required for **introducing non-linearity** to neural networks.\n",
        "\n",
        "Looking carefully at the equation for perceptron, we can see that **w*x + b** is a linear equation.\n",
        "\n",
        "Why do we need non-linearity? To model non-linear decision boundaries. We will cover this in detail later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "v9DreiAMqlBJ"
      },
      "outputs": [],
      "source": [
        "\"\"\"Implementing common activation functions. Expected input to all the functions are numpy arrays\"\"\"\n",
        "class ActivationFunctions:\n",
        "  @staticmethod\n",
        "  def linear(x):\n",
        "    \"\"\"Linear activation function: f(x) = x\n",
        "    Args:\n",
        "      x: np.ndarray : input to the activation function\n",
        "    \n",
        "    Returns:\n",
        "      np.ndarray\n",
        "    \"\"\"\n",
        "    return x\n",
        "\n",
        "  @staticmethod\n",
        "  def sigmoid(x):\n",
        "    \"\"\"Sigmoid activation function: f(x) = 1 / (1 + exp(-x))\n",
        "    \n",
        "    Args:\n",
        "      x: np.ndarray : input to the activation function\n",
        "    \n",
        "    Returns:\n",
        "      np.ndarray\n",
        "    \"\"\"\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "  @staticmethod\n",
        "  def relu(x):\n",
        "    \"\"\"ReLU activation function: f(x) = max(0, x)\n",
        "    \n",
        "    Args:\n",
        "      x: np.ndarray : input to the activation function\n",
        "    \n",
        "    Returns:\n",
        "      np.ndarray\n",
        "    \"\"\"\n",
        "    # return max(0, x) this won't work for an array\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "  @staticmethod\n",
        "  def tanh(x):\n",
        "    \"\"\"Tanh activation function: f(x) = (exp(2x) - 1) / (exp(2x) + 1)\n",
        "    \n",
        "    Args:\n",
        "      x: np.ndarray : input to the activation function\n",
        "    \n",
        "    Returns:\n",
        "      np.ndarray\n",
        "    \"\"\"\n",
        "    return (np.exp(2*x) - 1) / (np.exp(2*x) + 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1wbWrIBlstEG"
      },
      "source": [
        "## Batch inputs\n",
        "\n",
        "Lets now run this for a batch (set of inputs).\n",
        "We will create a batch of 5 scalar inputs to run through our neuron.\n",
        "\n",
        "Since inputs are represented as column vector => each input is a new column in the input matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gV0CuQDiAtKU",
        "outputId": "e1f841c1-d357-435b-f783-2e3f44a26479"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "x = [7 8 1 1 4]\n",
            "y_pred = [22 25  4  4 13]\n"
          ]
        }
      ],
      "source": [
        "# running above calculations for a batch of scalar inputs. We will draw a sample of 5 inputs with values between 1 and 10\n",
        "batch_size = 5\n",
        "\n",
        "x = np.random.randint(1, 10, batch_size)\n",
        "print(\"x =\", x)\n",
        "\n",
        "# we added activation function here for now. We will move it to the Perceptron class definition later\n",
        "y_pred = ActivationFunctions.linear(perceptron1.forward(x))\n",
        "print(\"y_pred =\", y_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQ7PDhxD0XfL"
      },
      "source": [
        "## Multi-dimensional inputs\n",
        "\n",
        "Lets add more dimensions to input. As we increase the dimensions of either input or output, we switch to vector form of the perceptron equation.  \n",
        "\n",
        "* x_input is represented as column vector\n",
        "```\n",
        "Ex 1 - two dimensional input sample\n",
        "2\n",
        "1\n",
        "Ex - 3 input samples of 2 dimension will be represented as\n",
        "2 3 1\n",
        "1 0 3\n",
        "```\n",
        "\n",
        "* weight represented as an **output_neurons * input_neurons** matrix\n",
        "* bias a column vector {output x 1}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sSDIwTm-AgeQ",
        "outputId": "4c8916da-03f9-4210-b375-539810510a5e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "x =\n",
            " [[2]\n",
            " [2]\n",
            " [0]]\n",
            "\n",
            "weights =\n",
            " [[0 0 0]]\n",
            "\n",
            "y_pred =\n",
            " [[1]]\n"
          ]
        }
      ],
      "source": [
        "# Now lets move to multi-dimensional input\n",
        "# lets say input features are 3 so input has 3 rows and 1 columns, and weight has 1 row and 3 columns\n",
        "# and start with single perceptron calculation\n",
        "\n",
        "input_dimensions = 3 # since we decided for a 3 dimensional input above\n",
        "output_dimensions = 1 # since we have only 1 neuron so only 1 output\n",
        "\n",
        "# we are specifying 2 dimensions for input because we want a column vector, other way to achieve same is by creating row vectors and transposing them\n",
        "x = np.random.randint(0, 5, size=(input_dimensions, 1))\n",
        "print(\"x =\\n\", x)\n",
        "\n",
        "# create weight vectors with dimensions as shared above\n",
        "weight = np.random.randint(0, 2, size=(output_dimensions, input_dimensions))\n",
        "print(\"\\nweights =\\n\", weight)\n",
        "# we can have bias as a 1X1 matrix or scalar both works\n",
        "bias = 1\n",
        "\n",
        "# Note that we switched to using np.matmul instead of * (scalar multiplier) for multiplication\n",
        "y_pred = ActivationFunctions.linear(np.matmul(weight, x) + bias)\n",
        "print(\"\\ny_pred =\\n\", y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "x =\n",
            " [[2]\n",
            " [2]\n",
            " [0]]\n",
            "\n",
            "y_pred =\n",
            " [[1]]\n"
          ]
        }
      ],
      "source": [
        "# lets vectorize the perceptron class and add activation function to it\n",
        "class Perceptron:\n",
        "    \"\"\"\n",
        "    A simple perceptron implementation.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, weight, bias, activation_function):\n",
        "        \"\"\"\n",
        "        Initialize the perceptron with the given weight and bias.\n",
        "        \n",
        "        Args:\n",
        "            weight (np.ndarray) : The weight of the perceptron.\n",
        "            bias (float/int/np.ndarray) : The bias of the perceptron.\n",
        "            activation_function (function) : The activation function to use.\n",
        "        \"\"\"\n",
        "        self.weight = weight\n",
        "        self.bias = bias\n",
        "        self.activation_function = activation_function\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Calculate the output of the perceptron for the given input.\n",
        "        \n",
        "        Args:\n",
        "            x (np.ndarray): The input to the perceptron.\n",
        "    \n",
        "        Returns:\n",
        "            np.ndarray: The output of the perceptron.\n",
        "        \"\"\"\n",
        "        return self.activation_function(np.matmul(self.weight, x) + self.bias)\n",
        "    \n",
        "# lets create an object of the perceptron class and test it using the same set of inputs, weight and bias as above\n",
        "perceptron1 = Perceptron(weight, bias, ActivationFunctions.linear)\n",
        "print(\"x =\\n\", x)\n",
        "y_pred = perceptron1.forward(x)\n",
        "print(\"\\ny_pred =\\n\", y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "x =\n",
            " [[0 2 0 2 1]\n",
            " [3 0 4 4 2]\n",
            " [3 4 1 4 4]]\n",
            "\n",
            "y_pred =\n",
            " [[1 1 1 1 1]]\n"
          ]
        }
      ],
      "source": [
        "# lets test the above Perceptron class for a batch of inputs\n",
        "batch_size = 5\n",
        "\n",
        "x = np.random.randint(0, 5, size=(input_dimensions, batch_size))\n",
        "print(\"x =\\n\", x)\n",
        "\n",
        "# we are reusing the same object of perceptron class that we created above => no change in weight and bias\n",
        "y_pred = perceptron1.forward(x)\n",
        "print(\"\\ny_pred =\\n\", y_pred)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GnPw9OvE83DT"
      },
      "source": [
        "## Multi-dimensional output\n",
        "\n",
        "So far we were looking at single neuron case, what if I add more neuron to this setup. In case of multiple output neurons, we are going to have multiple outputs for each input sample.\n",
        "\n",
        "Lets first try to understand what does having multiple output neurons imply. So far we have seen that a single neuron had -\n",
        "1. a set of weights\n",
        "2. a bias term\n",
        "3. activation function\n",
        "4. output\n",
        "\n",
        "So adding a new neuron implies that we need to add weights, bias and activation function for this new neuron & this addition will lead to addition of one more dimension to the output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gIz4SkZMCA1X",
        "outputId": "ddf7d953-94fd-4c3f-e850-2a23ac9b174b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "x =\n",
            " [[2]\n",
            " [1]\n",
            " [0]]\n",
            "\n",
            "weights =\n",
            " [[1 1 1]\n",
            " [0 1 1]]\n",
            "\n",
            "bias =\n",
            " [[0]\n",
            " [0]]\n",
            "\n",
            "y_pred =\n",
            " [[3]\n",
            " [1]]\n"
          ]
        }
      ],
      "source": [
        "# Now lets move to multi-dimensional output as in add 1 more neuron to our setup => 2 outputs\n",
        "# this new neuron need to be initialized with new set of weights, bias and activation function\n",
        "# continuing our earlier example where we had 3 input neurons and now 2 output neurons\n",
        "\n",
        "input_dimensions = 3 # since we decided for a 3 dimensional input above\n",
        "output_dimensions = 2 # since we now have 2 neuron so 2-dimensional output is expected now\n",
        "\n",
        "# we are specifying 2 dimensions for input because we want a column vector, other way to achieve same is by creating row vectors and transposing them\n",
        "x = np.random.randint(0, 5, size=(input_dimensions, 1))\n",
        "print(\"x =\\n\", x)\n",
        "\n",
        "# create weight vectors with dimensions as shared above\n",
        "weight = np.random.randint(0, 2, size=(output_dimensions, input_dimensions))\n",
        "print(\"\\nweights =\\n\", weight)\n",
        "\n",
        "# here we can't continue with scalar bias because now we have 2 neurons with their independent bias terms\n",
        "# so we need a column vector with dimensions as output_dimensions X 1\n",
        "bias = np.random.randint(-1, 1, size=(output_dimensions, 1))\n",
        "print(\"\\nbias =\\n\", bias)\n",
        "\n",
        "# Note that we don't need to change the perceptron class for this change in output dimensions\n",
        "# its already designed to handle any number of output dimensions\n",
        "perceptron2 = Perceptron(weight, bias, ActivationFunctions.linear)\n",
        "y_pred = perceptron2.forward(x)\n",
        "print(\"\\ny_pred =\\n\", y_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YR4yBYBICCLp"
      },
      "source": [
        "## Batch Input + Multi-dimensional input and output\n",
        "\n",
        "Now we want to write code that can run multiple input samples through our neural network in one go. Also this time we have a slightly complex network consisting of multiple neurons and multi-dimensional inputs.\n",
        "\n",
        "In the above cell, we laid foundations to build multi-neuron network, so lets start with that and identify if there are any changes required for batch inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Me9h4eDBCVcD",
        "outputId": "65061693-33cc-428d-b55b-686bb2e2169b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "x =\n",
            " [[3 0 0 0 2]\n",
            " [0 2 1 0 4]\n",
            " [0 2 3 3 3]]\n",
            "\n",
            "weights =\n",
            " [[0 1 1]\n",
            " [0 0 1]]\n",
            "\n",
            "bias =\n",
            " [[ 0]\n",
            " [-1]]\n",
            "\n",
            "y_pred =\n",
            " [[ 0  4  4  3  7]\n",
            " [-1  1  2  2  2]]\n"
          ]
        }
      ],
      "source": [
        "# continuing our earlier example where we had 3 input neurons, 2 output neurons\n",
        "# and now adding 5 input samples instead of 1\n",
        "\n",
        "batch_size = 5 # number of input samples we want to process\n",
        "input_dimensions = 3 # since we decided for a 3 dimensional input above\n",
        "output_dimensions = 2 # since we now have 2 neuron so 2-dimensional output is expected now\n",
        "\n",
        "# we are specifying 2 dimensions for input because we want a column vector, other way to achieve same is by creating row vectors and transposing them\n",
        "# Note the change in dimension of input\n",
        "x = np.random.randint(0, 5, size=(input_dimensions, batch_size))\n",
        "print(\"x =\\n\", x)\n",
        "\n",
        "# create weight vectors with dimensions as shared above\n",
        "# Note no change in weights and bias as no change in underlying neural network structure\n",
        "weight = np.random.randint(0, 2, size=(output_dimensions, input_dimensions))\n",
        "print(\"\\nweights =\\n\", weight)\n",
        "\n",
        "# here we can't continue with scalar bias because now we have 2 neurons with their independent bias terms\n",
        "# so we need a column vector with dimensions as output_dimensions X 1\n",
        "bias = np.random.randint(-1, 1, size=(output_dimensions, 1))\n",
        "print(\"\\nbias =\\n\", bias)\n",
        "\n",
        "# Note that we don't need to change the perceptron class for this as well\n",
        "perceptron2 = Perceptron(weight, bias, ActivationFunctions.linear)\n",
        "y_pred = perceptron2.forward(x)\n",
        "print(\"\\ny_pred =\\n\", y_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ySaJJtQfamu8"
      },
      "source": [
        "## Neural Network Layer\n",
        "\n",
        "A neural network layer comprises of one or more neurons connected to same set of inputs and outputs.\n",
        "\n",
        "If you look back, even when we coded a single neuron or multiple neurons above - we are effectively building a layer in the network.\n",
        "\n",
        "To define a neural network, we need to know following -\n",
        "1.  number of input neurons\n",
        "2.  number of output neurons\n",
        "3.  activation function for the neurons in this layer\n",
        "\n",
        "Input and output neuron count help us identify the dimensions of weight and bias matrix. The code below is just tweaking Perceptron class to work with a different set of input parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "nlKGR2OKfGLx"
      },
      "outputs": [],
      "source": [
        "class NeuralNetworkLayer:\n",
        "  \"\"\"\n",
        "  A simple neural network layer implementation.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, input_dim: int, output_dim: int, act_function: callable, is_bias: bool = True) -> None:\n",
        "    \"\"\"\n",
        "    Initialize the layer with input and output dimensions, activation function\n",
        "    Weights will be initialized randomly for now (binary values)\n",
        "    Bias will be initialized randomly for now (-1, 0, 1)\n",
        "\n",
        "    Args:\n",
        "      input_dim (int) : Number of input dimensions\n",
        "      output_dim (int) : Number of output dimensions\n",
        "      act_function (function) : Activation function to use\n",
        "      is_bias (bool) : Whether to use bias or not\n",
        "    \"\"\"\n",
        "    self.input_dimensions = input_dim\n",
        "    self.output_dimensions = output_dim\n",
        "    self.activation_function = act_function\n",
        "\n",
        "    # For now we can initialize weights randomly to start with\n",
        "    # we will deep dive later on how to set initial weights while covering the model training\n",
        "    self.weights = np.random.randint(0, 2, size=(self.output_dimensions, self.input_dimensions))\n",
        "    if is_bias:\n",
        "      self.bias = np.random.randint(-1, 2, size=(self.output_dimensions, 1))\n",
        "    else:\n",
        "      self.bias = np.zeros((self.output_dimensions, 1))\n",
        "\n",
        "\n",
        "  def predict(self, input:np.ndarray) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Calculate the output of the layer for the given input.\n",
        "\n",
        "    Args:\n",
        "      input (np.ndarray): The input to the layer.\n",
        "\n",
        "    Returns:\n",
        "      np.ndarray: The output of the layer.\n",
        "    \"\"\"\n",
        "    return self.activation_function(np.matmul(self.weights, input) + self.bias)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mN3-XgOxgJ4C"
      },
      "source": [
        "Pytorch implementation of linear layer is available [here](https://github.com/pytorch/pytorch/blob/3a185778edb18abfbad155a87ff3b2d716e4c220/torch/nn/modules/linear.py#L93)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "VSen4-VYFK3r"
      },
      "outputs": [],
      "source": [
        "class InputUtils:\n",
        "  def __init__(self, input_dimensions):\n",
        "    self.input_dimensions = input_dimensions\n",
        "\n",
        "  def getInput(self):\n",
        "    return self.getInputBatch(1)\n",
        "\n",
        "  def getInputBatch(self, batch_size):\n",
        "    x = np.random.randint(0, 5, size=(self.input_dimensions, batch_size))\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5r-mx3LDgmxy"
      },
      "source": [
        "Lets use the code that we created above to create a linear neural network layer with 3 inputs and 2 outputs.\n",
        "\n",
        "Linear layer is nothing but one which maps inputs (x) to output (y) using following relationship -\n",
        "\n",
        "y = w.x + b\n",
        "\n",
        "Linear layer implicitly uses linear activation function.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "00BMiNpldhty",
        "outputId": "4b6d7a50-ccda-4920-de5a-0671a595cc45"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "x_input = \n",
            " [[2 1 0 1 0]\n",
            " [3 1 1 3 0]\n",
            " [2 0 3 2 2]]\n",
            "\n",
            "weights = \n",
            " [[0 1 1]\n",
            " [1 1 1]]\n",
            "\n",
            "bias = \n",
            " [[ 0]\n",
            " [-1]]\n",
            "\n",
            "y_pred = \n",
            " [[5 1 4 5 2]\n",
            " [6 1 3 5 1]]\n"
          ]
        }
      ],
      "source": [
        "input_dimensions = 3\n",
        "output_dimensions = 2\n",
        "\n",
        "input_sampler = InputUtils(input_dimensions)\n",
        "layer = NeuralNetworkLayer(input_dimensions, output_dimensions, ActivationFunctions.linear, is_bias=True)\n",
        "\n",
        "x_input = input_sampler.getInputBatch(5)\n",
        "print(\"x_input = \\n\", x_input)\n",
        "\n",
        "print(\"\\nweights = \\n\", layer.weights)\n",
        "\n",
        "print(\"\\nbias = \\n\", layer.bias)\n",
        "\n",
        "print(\"\\ny_pred = \\n\", layer.predict(x_input))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wXt5f2xl4tfm"
      },
      "source": [
        "While different neurons in the same layer can have different activation functions, the usual practice is to use same activation function for all the neuron in a layer. Check this to know [more](https://datascience.stackexchange.com/questions/72559/different-activation-function-in-same-layer-of-a-neural-network)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yxlt8YD7Xrd_"
      },
      "source": [
        "# Recap\n",
        "\n",
        "This wraps up this notebook. Just to recap we learned -\n",
        "\n",
        "\n",
        "1.   Artificial Neuron and its mathematical representation\n",
        "2.   How to code a\n",
        "    \n",
        "        *   Neuron processing single 1-D input\n",
        "        *   Neuron processing multiple 1-D input (Batch Inputs)\n",
        "        *   Neuron processing multiple n-D inputs\n",
        "        *   NeuralNetworkLayer consisting of multiple neurons"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lVyZOIeimr34"
      },
      "source": [
        "# References"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5puTbYau9hpq"
      },
      "source": [
        "1.   https://aibyhand.substack.com/"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
