{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LixRLTwzMBsZ"
      },
      "source": [
        "The idea behind this notebook series is to go slow and build a solid understanding of code and maths involved in deep learning. Few points to note -\n",
        "1. For simplicity of calculations, I have used small integer values in examples\n",
        "2. This code is only for understanding the concepts so it is missing couple of things like type checking and error handling\n",
        "\n",
        "This is the 1st notebook in the series. Here we start with a single neuron (scalar input and output) and build upto the idea of a network layer.\n",
        "\n",
        "[Colab link](https://colab.research.google.com/github/shwetaAgrawal/deeplearning_tutorials/blob/main/notebooks/1_Intro_to_perceptron.ipynb)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Pre-requisite for running this notebook - numpy. If you come across error \"No module named 'numpy'\" then please uncomment the below line and run this cell\n",
        "#!pip install numpy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "osEjQ7iSmFKp"
      },
      "source": [
        "# Introduction to Neural Networks\n",
        "\n",
        "Neural networks are composed of multiple nodes knows as neurons. Like any network node, neurons also have incoming and outgoing connections. All the connections in neural networks are weighted. During training of a neural network, we try to find the optimal weights for given data (input features and target). \n",
        "\n",
        "![Neural Network](assets/images/NeuralNetworkIllustration.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hSb87NDa9tQ9"
      },
      "source": [
        "## Neuron \n",
        "Lets start by understanding working of a neuron\n",
        "\n",
        "How does an artifical neuron work?\n",
        "It takes in an **input** - applies **weight** and **intercept** followed by an **activation** function\n",
        "$$\n",
        "\\hat y = f(W^T \\cdot x + b)\n",
        "$$\n",
        "- where ${f}$ is activation function\n",
        "- W is weight matrix consisting of individual neurons weight vectors. A single neuron's weight vector is also represented as a column vector with number of rows = number of input dimensions\n",
        "- x is input vector usually depicted as a column vector.\n",
        "- b is bias\n",
        "\n",
        "![Single Neuron](assets/images/SingleNeuron.png)\n",
        "\n",
        "In this notebook, we will compute ${\\hat y}$ for a given input and weight. We will be increasing the complexity slowly till we are able to build a full network consisting of multiple layers and able to support multi-dimensional inputs and outputs.\n",
        "\n",
        "This notebook follows the concepts covered in [Linear Layer Worksheet by Tom Yeh](https://aibyhand.substack.com/p/w3-linear-layer). If you are like me, you might want to grab the pen and notebook to do the exercises covered in the linked workbook before going through code here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "3SDoNbxR9f04"
      },
      "outputs": [],
      "source": [
        "# numpy is used for efficient numerical operations - it supports vectorized operations on n-dimensional arrays\n",
        "# random is used to generate random numbers for generating input data\n",
        "import numpy as np\n",
        "import random"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4rTbuztX1MAx"
      },
      "source": [
        "### Neuron with 1-D input and 1-D output \n",
        "\n",
        "1-D input => input vector of dimension 1 x 1\n",
        "\n",
        "Now since there is 1-D input - our weight matrix's dimensions are also 1 x 1 and transpose of this matrix is matrix itself so ${W^T = W}$\n",
        "\n",
        "To keep things simple, we will represent this 1 X 1 matrices as scalar. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X4wtY3riqnYE",
        "outputId": "f9ef256d-3692-427b-bcf0-1eef9006a143"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input, x = 1\n",
            "Prediction, y_pred = 4\n"
          ]
        }
      ],
      "source": [
        "# Single neuron with 1-D input and 1-D output. As mentioned above, to keep it simple, we are representing the 1x1 matrix as a scalar value\n",
        "x = random.randint(1, 10)\n",
        "print(\"Input, x =\", x)\n",
        "weight = 3\n",
        "bias = 1\n",
        "\n",
        "# lets say activation function is linear f(x) = x for all values of x and transpose of a scalar is scalar itself\n",
        "y_pred = (weight * x + bias)\n",
        "print(\"Prediction, y_pred =\", y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input, x = 1\n",
            "Prediction, y_pred = 4\n"
          ]
        }
      ],
      "source": [
        "#Lets define the neuron class \n",
        "class ArtificalNeuron:\n",
        "    \"\"\"\n",
        "    A simple artificial neuron implementation.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, weight: np.ndarray, bias: np.ndarray) -> None:\n",
        "        \"\"\"\n",
        "        Initialize the neuron with the given weight and bias.\n",
        "        \n",
        "        Args:\n",
        "            weight (float/int) : The weight of the neuron.\n",
        "            bias (float/int) : The bias of the neuron.\n",
        "        \"\"\"\n",
        "        self.weight = weight\n",
        "        self.bias = bias\n",
        "\n",
        "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Calculate the output of the neuron for the given input.\n",
        "        \n",
        "        Args:\n",
        "            x (float/int/np.ndarray): The input to the neuron.\n",
        "    \n",
        "        Returns:\n",
        "            float/int/np.ndarray: The output of the neuron.\n",
        "        \"\"\"\n",
        "        return self.weight * x + self.bias\n",
        "    \n",
        "# Lets create an object of the neuron class and test it\n",
        "neuron = ArtificalNeuron(weight, bias)\n",
        "\n",
        "# Lets test using the same input x that we used in earlier cell\n",
        "print(\"Input, x =\", x)\n",
        "y_pred = neuron.forward(x)\n",
        "print(\"Prediction, y_pred =\", y_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HOo80Qo72BO4"
      },
      "source": [
        "### Activation Functions\n",
        "\n",
        "Lets define some common activation functions. Activation functions are used for **introducing non-linearity** to neural networks. Looking carefully at the equation for the artificial neuron, we can see that ${W^T \\cdot x + b}$ is a linear equation so we need non-linearity to be able to model different types of deep learning tasks which can have linear as well as non-linear decision boundaries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "v9DreiAMqlBJ"
      },
      "outputs": [],
      "source": [
        "\"\"\"Implementing common activation functions. Expected input to all the functions are numpy arrays\"\"\"\n",
        "class ActivationFunctions:\n",
        "  @staticmethod\n",
        "  def linear(x):\n",
        "    \"\"\"Linear activation function: f(x) = x\n",
        "    Args:\n",
        "      x: np.ndarray : input to the activation function\n",
        "    \n",
        "    Returns:\n",
        "      np.ndarray\n",
        "    \"\"\"\n",
        "    return x\n",
        "\n",
        "  @staticmethod\n",
        "  def sigmoid(x):\n",
        "    \"\"\"Sigmoid activation function: f(x) = 1 / (1 + exp(-x))\n",
        "    \n",
        "    Args:\n",
        "      x: np.ndarray : input to the activation function\n",
        "    \n",
        "    Returns:\n",
        "      np.ndarray\n",
        "    \"\"\"\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "  @staticmethod\n",
        "  def relu(x):\n",
        "    \"\"\"ReLU activation function: f(x) = max(0, x)\n",
        "    \n",
        "    Args:\n",
        "      x: np.ndarray : input to the activation function\n",
        "    \n",
        "    Returns:\n",
        "      np.ndarray\n",
        "    \"\"\"\n",
        "    # return max(0, x) this won't work for an array\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "  @staticmethod\n",
        "  def tanh(x):\n",
        "    \"\"\"Tanh activation function: f(x) = (exp(2x) - 1) / (exp(2x) + 1)\n",
        "    \n",
        "    Args:\n",
        "      x: np.ndarray : input to the activation function\n",
        "    \n",
        "    Returns:\n",
        "      np.ndarray\n",
        "    \"\"\"\n",
        "    return (np.exp(2*x) - 1) / (np.exp(2*x) + 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1wbWrIBlstEG"
      },
      "source": [
        "### Batch input processing with a neuron\n",
        "\n",
        "Lets now run our existing code for a batch (set of inputs). We will create a batch of 5 scalar inputs to run through our neuron.\n",
        "\n",
        "Since inputs are represented as column vector => each input is a new column in the input matrix => input matrix dimensions = 1 x 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gV0CuQDiAtKU",
        "outputId": "e1f841c1-d357-435b-f783-2e3f44a26479"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "x = [2 2 2 6 2]\n",
            "Weight = 3\n",
            "Bias = 1\n",
            "y_pred = [ 7  7  7 19  7]\n"
          ]
        }
      ],
      "source": [
        "# running above calculations for a batch of scalar inputs. We will draw a sample of 5 inputs with values between 1 and 10\n",
        "batch_size = 5\n",
        "\n",
        "x = np.random.randint(1, 10, batch_size)\n",
        "print(\"x =\", x)\n",
        "print(\"Weight =\", neuron.weight)\n",
        "print(\"Bias =\", neuron.bias)\n",
        "\n",
        "# we added activation function here for now. We will move it to the ArtificalNeuron class definition later\n",
        "y_pred = ActivationFunctions.linear(neuron.forward(x))\n",
        "print(\"y_pred =\", y_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "** Our existing class can handle both batch of inputs and a single input **"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQ7PDhxD0XfL"
      },
      "source": [
        "### Neuron with Multi-dimensional inputs\n",
        "\n",
        "Lets increase complexity by adding more dimensions to input and analyze how our code changes with this change. \n",
        "\n",
        "We will start with understanding representation changes if any required for multi-dimensional input. We know that ${x}$ is represented as column vector, so an n-dimensional input will be represented as n x 1 matrix.\n",
        "```\n",
        "Ex 1 - two dimensional input sample\n",
        "2\n",
        "1\n",
        "Ex - 3 input samples of 2 dimension will be represented as\n",
        "2 3 1\n",
        "1 0 3\n",
        "```\n",
        "weight matrix is also represented as column vector and number of weights for a neuron == number of incoming input connections => weight matrix will also have n x 1 dimension. Now, because of increase in dimensions, ${W^T \\not= W}$, so we will need to make this change in our original neuron class.\n",
        "\n",
        "As there is still 1 neuron, we can continue to represent bias as 1 x 1 matrix or a scalar. Usually bias is a vector with dimension = **num_neurons x 1**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sSDIwTm-AgeQ",
        "outputId": "4c8916da-03f9-4210-b375-539810510a5e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "x =\n",
            " [[2]\n",
            " [2]\n",
            " [1]]\n",
            "\n",
            "weights =\n",
            " [[0]\n",
            " [1]\n",
            " [0]]\n",
            "\n",
            "y_pred =\n",
            " [[3]]\n"
          ]
        }
      ],
      "source": [
        "# Now lets move to multi-dimensional input\n",
        "# lets say input features are 3 so input has 3 rows and 1 columns, and weight is also 3 rows and 1 column\n",
        "# bias continues to be a scalar\n",
        "input_dimensions = 3 # since we decided for a 3 dimensional input above\n",
        "output_dimensions = 1 # since we have only 1 neuron so only 1 output\n",
        "\n",
        "# we are specifying 2 dimensions for input because we want a column vector, other way to achieve same is by creating row vectors and transposing them\n",
        "x = np.random.randint(0, 5, size=(input_dimensions, 1))\n",
        "print(\"x =\\n\", x)\n",
        "\n",
        "# create weight vectors with dimensions as shared above\n",
        "weight = np.random.randint(0, 2, size=(input_dimensions, output_dimensions))\n",
        "print(\"\\nweights =\\n\", weight)\n",
        "# we can have bias as a 1 x 1 matrix or scalar both works\n",
        "bias = 1\n",
        "\n",
        "# Note that we switched to using np.matmul instead of * (scalar multiplier) for multiplication \n",
        "# We also transposed the weight matrix to match with our original definition \n",
        "y_pred = ActivationFunctions.linear(np.matmul(weight.T, x) + bias)\n",
        "print(\"\\ny_pred =\\n\", y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "x =\n",
            " [[2]\n",
            " [2]\n",
            " [1]]\n",
            "weights =\n",
            " [[0]\n",
            " [1]\n",
            " [0]]\n",
            "bias = 1\n",
            "\n",
            "y_pred =\n",
            " [[3]]\n"
          ]
        }
      ],
      "source": [
        "# lets vectorize the ArtificialNeuron class and add activation function to it\n",
        "class Neuron:\n",
        "    \"\"\"\n",
        "    A simple neuron implementation.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, weight, bias, activation_function):\n",
        "        \"\"\"\n",
        "        Initialize the neuron with the given weight and bias.\n",
        "        \n",
        "        Args:\n",
        "            weight (np.ndarray) : The weight of the neuron.\n",
        "            bias (float/int/np.ndarray) : The bias of the neuron.\n",
        "            activation_function (function) : The activation function to use.\n",
        "        \"\"\"\n",
        "        self.weight = weight\n",
        "        self.bias = bias\n",
        "        self.activation_function = activation_function\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Calculate the output of the neuron for the given input.\n",
        "        \n",
        "        Args:\n",
        "            x (np.ndarray): The input to the neuron.\n",
        "    \n",
        "        Returns:\n",
        "            np.ndarray: The output of the neuron.\n",
        "        \"\"\"\n",
        "        # Note that we switched to using np.matmul instead of * (scalar multiplier) for multiplication\n",
        "        # Note that we took a transpose of the weight matrix to support multi-dimensional input\n",
        "        return self.activation_function(np.matmul(self.weight.T, x) + self.bias)\n",
        "    \n",
        "# lets create an object of the neuron class and test it using the same set of inputs, weight and bias as above\n",
        "multi_input_neuron = Neuron(weight, bias, ActivationFunctions.linear)\n",
        "print(\"x =\\n\", x)\n",
        "print(\"weights =\\n\", weight)\n",
        "print(\"bias =\", bias)\n",
        "y_pred = multi_input_neuron.forward(x)\n",
        "print(\"\\ny_pred =\\n\", y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "x =\n",
            " [[1 0 0 3 0]\n",
            " [2 1 2 3 0]\n",
            " [3 0 2 2 1]]\n",
            "weights =\n",
            " [[0]\n",
            " [1]\n",
            " [0]]\n",
            "bias = 1\n",
            "\n",
            "y_pred =\n",
            " [[3 2 3 4 1]]\n"
          ]
        }
      ],
      "source": [
        "# lets try the above Neuron class for a batch of inputs\n",
        "batch_size = 5\n",
        "\n",
        "x = np.random.randint(0, 5, size=(input_dimensions, batch_size))\n",
        "print(\"x =\\n\", x)\n",
        "print(\"weights =\\n\", weight)\n",
        "print(\"bias =\", bias)\n",
        "\n",
        "# we are reusing the same object of ArtificalNeuron class that we created above => no change in weight and bias\n",
        "y_pred = multi_input_neuron.forward(x)\n",
        "print(\"\\ny_pred =\\n\", y_pred)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GnPw9OvE83DT"
      },
      "source": [
        "## Neural Network Layer - Multiple Neurons with same input\n",
        "\n",
        "Neural network layer consists of neurons having same set of inputs. So if we add one more neuron to our earlier setup with same inputs, we get a neural network layer. Now since we have multiple output neurons, we are going to have multiple outputs for each input record. Our output is also represented as a column vector with dimensions = **num_neurons x 1**\n",
        "\n",
        "Lets first try to understand what does having multiple output neurons imply. So far we have seen that a single neuron had -\n",
        "1. a set of weights\n",
        "2. a bias term\n",
        "3. activation function\n",
        "4. output\n",
        "\n",
        "So adding a new neuron implies that we need to add weights, bias and activation function for this new neuron & this addition will lead to addition of \n",
        "* one more column in the weight matrix representing new neuron's weights\n",
        "* one more row in the bias matrix representing new neuron's bias\n",
        "* output matrix dimension will now change to **num_neurons x 1**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gIz4SkZMCA1X",
        "outputId": "ddf7d953-94fd-4c3f-e850-2a23ac9b174b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "x =\n",
            " [[0]\n",
            " [2]\n",
            " [4]]\n",
            "\n",
            "weights =\n",
            " [[0 1]\n",
            " [1 1]\n",
            " [0 0]]\n",
            "\n",
            "bias =\n",
            " [[-1]\n",
            " [-1]]\n",
            "\n",
            "y_pred =\n",
            " [[1]\n",
            " [1]]\n"
          ]
        }
      ],
      "source": [
        "# Now lets move to multi-dimensional output as in add 1 more neuron to our setup => 2 outputs\n",
        "# this new neuron need to be initialized with new set of weights, bias and activation function\n",
        "# continuing our earlier example where we had 3 input neurons and now 2 output neurons\n",
        "\n",
        "input_dimensions = 3 # since we decided for a 3 dimensional input above\n",
        "output_dimensions = 2 # since we now have 2 neuron so 2-dimensional output is expected now\n",
        "\n",
        "# we are specifying 2 dimensions for input because we want a column vector, other way to achieve same is by creating row vectors and transposing them\n",
        "x = np.random.randint(0, 5, size=(input_dimensions, 1))\n",
        "print(\"x =\\n\", x)\n",
        "\n",
        "# create weight vectors with dimensions as shared above\n",
        "weight = np.random.randint(0, 2, size=(input_dimensions, output_dimensions))\n",
        "print(\"\\nweights =\\n\", weight)\n",
        "\n",
        "# here we can't continue with scalar bias because now we have 2 neurons with their independent bias terms\n",
        "# so we need a column vector with dimensions as output_dimensions X 1\n",
        "bias = np.random.randint(-1, 1, size=(output_dimensions, 1))\n",
        "print(\"\\nbias =\\n\", bias)\n",
        "\n",
        "# Note that we don't need to change the ArtificialNeuron class for this change in output dimensions\n",
        "# its already designed to handle any number of output dimensions\n",
        "network_layer = Neuron(weight, bias, ActivationFunctions.linear)\n",
        "y_pred = network_layer.forward(x)\n",
        "print(\"\\ny_pred =\\n\", y_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YR4yBYBICCLp"
      },
      "source": [
        "### Neural Network Layer with batch of inputs\n",
        "\n",
        "Now we are going to write code that can run multiple input samples through our neural network layer in one go. Also this time we have a slightly complex network consisting of multiple neurons and multi-dimensional inputs.\n",
        "\n",
        "In the above cell, we laid foundations to build neural network layer, so lets start with that and identify if there are any changes required for batch inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Me9h4eDBCVcD",
        "outputId": "65061693-33cc-428d-b55b-686bb2e2169b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "x =\n",
            " [[2 3 0 1 2]\n",
            " [4 4 3 4 2]\n",
            " [0 0 1 2 3]]\n",
            "\n",
            "weights =\n",
            " [[0 1]\n",
            " [0 1]\n",
            " [1 1]]\n",
            "\n",
            "bias =\n",
            " [[ 0]\n",
            " [-1]]\n",
            "\n",
            "y_pred =\n",
            " [[0 0 1 2 3]\n",
            " [5 6 3 6 6]]\n"
          ]
        }
      ],
      "source": [
        "# continuing our earlier example where we had 3 input neurons, 2 output neurons\n",
        "# and now adding 5 input samples instead of 1\n",
        "\n",
        "batch_size = 5 # number of input samples we want to process\n",
        "input_dimensions = 3 # since we decided for a 3 dimensional input above\n",
        "output_dimensions = 2 # since we now have 2 neuron so 2-dimensional output is expected now\n",
        "\n",
        "# we are specifying 2 dimensions for input because we want a column vector, other way to achieve same is by creating row vectors and transposing them\n",
        "# Note the change in dimension of input\n",
        "x = np.random.randint(0, 5, size=(input_dimensions, batch_size))\n",
        "print(\"x =\\n\", x)\n",
        "\n",
        "# create weight vectors with dimensions as shared above\n",
        "# Note no change in weights and bias as no change in underlying neural network structure\n",
        "weight = np.random.randint(0, 2, size=(input_dimensions, output_dimensions))\n",
        "print(\"\\nweights =\\n\", weight)\n",
        "\n",
        "# here we can't continue with scalar bias because now we have 2 neurons with their independent bias terms\n",
        "# so we need a column vector with dimensions as output_dimensions X 1\n",
        "bias = np.random.randint(-1, 1, size=(output_dimensions, 1))\n",
        "print(\"\\nbias =\\n\", bias)\n",
        "\n",
        "# Note that we don't need to change the ArtificialNeuron class for this as well\n",
        "network_layer = Neuron(weight, bias, ActivationFunctions.linear)\n",
        "y_pred = network_layer.forward(x)\n",
        "print(\"\\ny_pred =\\n\", y_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ySaJJtQfamu8"
      },
      "source": [
        "### Neural Network Layer\n",
        "\n",
        "Now that we have understood the computations, lets tweak the input parameters to match the real world representation of artificial neurons and neural network layers. In practice, weights & bias are initialized randomly (mostly) and during training these are updated to minimize differences between ${y}$ and ${\\hat y}$. \n",
        "\n",
        "![Neural Network Layer](assets/images/NeuralNetworkLayer.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "nlKGR2OKfGLx"
      },
      "outputs": [],
      "source": [
        "class NeuralNetworkLayer:\n",
        "  \"\"\"\n",
        "  A simple neural network layer implementation.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, input_dim: int, output_dim: int, act_function: callable, is_bias: bool = True) -> None:\n",
        "    \"\"\"\n",
        "    Initialize the layer with input and output dimensions, activation function\n",
        "    Weights will be initialized randomly for now (binary values)\n",
        "    Bias will be initialized randomly for now (-1, 0, 1)\n",
        "\n",
        "    Args:\n",
        "      input_dim (int) : Number of input dimensions\n",
        "      output_dim (int) : Number of output dimensions\n",
        "      act_function (function) : Activation function to use\n",
        "      is_bias (bool) : Whether to use bias or not\n",
        "    \"\"\"\n",
        "    self.input_dimensions = input_dim\n",
        "    self.output_dimensions = output_dim\n",
        "    self.activation_function = act_function\n",
        "\n",
        "    # For now we can initialize weights randomly to start with\n",
        "    # we will deep dive later on how to set initial weights while covering the model training\n",
        "    self.weights = np.random.randint(0, 2, size=(self.input_dimensions, self.output_dimensions))\n",
        "    if is_bias:\n",
        "      self.bias = np.random.randint(-1, 2, size=(self.output_dimensions, 1))\n",
        "    else:\n",
        "      self.bias = np.zeros((self.output_dimensions, 1))\n",
        "\n",
        "\n",
        "  def forward(self, input:np.ndarray) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Calculate the output of the layer for the given input.\n",
        "\n",
        "    Args:\n",
        "      input (np.ndarray): The input to the layer.\n",
        "\n",
        "    Returns:\n",
        "      np.ndarray: The output of the layer.\n",
        "    \"\"\"\n",
        "    return self.activation_function(np.matmul(self.weights.T, input) + self.bias)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mN3-XgOxgJ4C"
      },
      "source": [
        "Pytorch implementation of linear layer is available [here](https://github.com/pytorch/pytorch/blob/3a185778edb18abfbad155a87ff3b2d716e4c220/torch/nn/modules/linear.py#L93)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "VSen4-VYFK3r"
      },
      "outputs": [],
      "source": [
        "class InputUtils:\n",
        "  def __init__(self, input_dimensions):\n",
        "    self.input_dimensions = input_dimensions\n",
        "\n",
        "  def getInput(self):\n",
        "    return self.getInputBatch(1)\n",
        "\n",
        "  def getInputBatch(self, batch_size):\n",
        "    x = np.random.randint(0, 5, size=(self.input_dimensions, batch_size))\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5r-mx3LDgmxy"
      },
      "source": [
        "Lets use the code that we created above to create a linear neural network layer with 3 inputs and 2 outputs.\n",
        "\n",
        "Linear layer is simply a layer with all neuron using linear activation function.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "00BMiNpldhty",
        "outputId": "4b6d7a50-ccda-4920-de5a-0671a595cc45"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "x_input = \n",
            " [[2 4 1 4 0]\n",
            " [3 1 0 0 0]\n",
            " [1 1 0 1 2]]\n",
            "\n",
            "weights = \n",
            " [[1 0]\n",
            " [0 0]\n",
            " [0 1]]\n",
            "\n",
            "bias = \n",
            " [[-1]\n",
            " [ 1]]\n",
            "\n",
            "y_pred = \n",
            " [[ 1  3  0  3 -1]\n",
            " [ 2  2  1  2  3]]\n"
          ]
        }
      ],
      "source": [
        "input_dimensions = 3\n",
        "output_dimensions = 2\n",
        "\n",
        "input_sampler = InputUtils(input_dimensions)\n",
        "layer = NeuralNetworkLayer(input_dimensions, output_dimensions, ActivationFunctions.linear, is_bias=True)\n",
        "\n",
        "x_input = input_sampler.getInputBatch(5)\n",
        "print(\"x_input = \\n\", x_input)\n",
        "\n",
        "print(\"\\nweights = \\n\", layer.weights)\n",
        "\n",
        "print(\"\\nbias = \\n\", layer.bias)\n",
        "\n",
        "print(\"\\ny_pred = \\n\", layer.forward(x_input))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wXt5f2xl4tfm"
      },
      "source": [
        "While different neurons in the same layer can have different activation functions, the usual practice is to use same activation function for all the neurons in a layer. Check this to know [more](https://datascience.stackexchange.com/questions/72559/different-activation-function-in-same-layer-of-a-neural-network)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Neural Network\n",
        "\n",
        "A neural network comprises of one or more neural network layers stacked to predict final output.\n",
        "\n",
        "To define a neural network, we need to know following -\n",
        "1.  number of input neurons\n",
        "2.  number of output neurons\n",
        "3.  number of neurons and activation function for each hidden layer\n",
        "\n",
        "Input and output neuron count help us identify the dimensions of weight and bias matrix. We'll try to build a network first using network layers and then write the end-to-end code for neural network calculation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "x_input = \n",
            " [[4 2 2 0 1]\n",
            " [1 4 4 1 4]\n",
            " [1 4 4 2 4]]\n",
            "\n",
            "weights = \n",
            " [[0 0]\n",
            " [0 0]\n",
            " [1 1]]\n",
            "\n",
            "bias = \n",
            " [[1]\n",
            " [0]]\n",
            "\n",
            "hidden_layer_output = \n",
            " [[2 5 5 3 5]\n",
            " [1 4 4 2 4]]\n",
            "\n",
            "weights = \n",
            " [[1 0]\n",
            " [1 1]]\n",
            "\n",
            "bias = \n",
            " [[-1]\n",
            " [ 0]]\n",
            "\n",
            "y_pred = \n",
            " [[2 8 8 4 8]\n",
            " [1 4 4 2 4]]\n"
          ]
        }
      ],
      "source": [
        "# Now lets move to multi-layer neural network\n",
        "# we will start with a simple 2 layer neural network\n",
        "# we will have 3 input neurons, 2 hidden neurons and 2 output neurons\n",
        "# we will use ReLU activation function for hidden layer and linear activation function for output layer\n",
        "\n",
        "input_dimensions = 3\n",
        "output_dimensions = 2\n",
        "num_hidden_neurons = 2\n",
        "\n",
        "input_sampler = InputUtils(input_dimensions)\n",
        "\n",
        "#create layer1, note output dimensions of layer1 will be input dimensions of layer2 and is equal to number of nodes in hidden layer\n",
        "hidden_layer = NeuralNetworkLayer(input_dimensions, num_hidden_neurons, ActivationFunctions.relu, is_bias=True)\n",
        "final_layer = NeuralNetworkLayer(num_hidden_neurons, output_dimensions, ActivationFunctions.relu, is_bias=True)\n",
        "\n",
        "x_input = input_sampler.getInputBatch(5)\n",
        "print(\"x_input = \\n\", x_input)\n",
        "\n",
        "print(\"\\nweights = \\n\", hidden_layer.weights)\n",
        "\n",
        "print(\"\\nbias = \\n\", hidden_layer.bias)\n",
        "\n",
        "print(\"\\nhidden_layer_output = \\n\", hidden_layer.forward(x_input))\n",
        "\n",
        "print(\"\\nweights = \\n\", final_layer.weights)\n",
        "print(\"\\nbias = \\n\", final_layer.bias)\n",
        "print(\"\\ny_pred = \\n\", final_layer.forward(hidden_layer.forward(x_input)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Now we can define a simple neural network class which will have multiple layers\n",
        "\n",
        "class NeuralNetwork:\n",
        "    def __init__(self, input_dimensions: int, output_dimensions: int, hidden_layer_neuron_count: list[int]) -> None:\n",
        "        \"\"\"\n",
        "        Initialize the neural network with input and output dimensions and number of neurons in each hidden layer\n",
        "        Weights will be initialized randomly for now (binary values)\n",
        "        Bias will be initialized randomly for now (-1, 0, 1)\n",
        "        Lets assume that we are using only ReLU activation function for now for all layers\n",
        "\n",
        "        Args:\n",
        "            input_dimensions (int) : Number of input dimensions\n",
        "            output_dimensions (int) : Number of output dimensions\n",
        "            hidden_layer_neuron_count (list[int]) : Number of neurons in each hidden layer\n",
        "        \"\"\"\n",
        "        self.input_dimensions = input_dimensions\n",
        "        self.output_dimensions = output_dimensions\n",
        "        self.hidden_layer_neuron_count = hidden_layer_neuron_count\n",
        "        self.layers = []\n",
        "        \n",
        "        tmp_input_dimensions = input_dimensions\n",
        "        for num_neuron in hidden_layer_neuron_count:\n",
        "            self.add_layer(tmp_input_dimensions, num_neuron)\n",
        "            tmp_input_dimensions = num_neuron\n",
        "        self.add_layer(tmp_input_dimensions, output_dimensions)\n",
        "\n",
        "    def add_layer(self, input_dimensions: int, output_dimensions: int) -> None:\n",
        "        \"\"\"\n",
        "        Add a layer to the neural network.\n",
        "\n",
        "        Args:\n",
        "            input_dimensions (int) : Number of input dimensions\n",
        "            output_dimensions (int) : Number of output dimensions\n",
        "        \"\"\"\n",
        "        if input_dimensions <= 0 or output_dimensions <= 0:\n",
        "            raise ValueError(\"Number of neurons in hidden layer should be greater than 0\")\n",
        "        self.layers.append(NeuralNetworkLayer(input_dimensions, output_dimensions, ActivationFunctions.relu, is_bias=True))\n",
        "\n",
        "    def forward(self, input: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Calculate the output of the neural network for the given input.\n",
        "\n",
        "        Args:\n",
        "            input (np.ndarray): The input to the neural network.\n",
        "\n",
        "        Returns:\n",
        "            np.ndarray: The output of the neural network.\n",
        "        \"\"\"\n",
        "        for layer in self.layers:\n",
        "            input = layer.forward(input)\n",
        "        return input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "x_input = \n",
            " [[1 4 0 2 0]\n",
            " [0 4 0 3 0]\n",
            " [1 4 4 4 3]]\n",
            "\n",
            "weights = \n",
            " [[0 1]\n",
            " [0 1]\n",
            " [0 0]]\n",
            "\n",
            "bias = \n",
            " [[0]\n",
            " [0]]\n",
            "\n",
            "hidden_layer_output = \n",
            " [[0 0 0 0 0]\n",
            " [1 8 0 5 0]]\n",
            "\n",
            "weights = \n",
            " [[0 0]\n",
            " [1 1]]\n",
            "\n",
            "bias = \n",
            " [[ 0]\n",
            " [-1]]\n",
            "\n",
            "y_pred = \n",
            " [[1 8 0 5 0]\n",
            " [0 7 0 4 0]]\n"
          ]
        }
      ],
      "source": [
        "input_dimensions = 3\n",
        "output_dimensions = 2\n",
        "num_hidden_neurons = 2\n",
        "\n",
        "input_sampler = InputUtils(input_dimensions)\n",
        "x_input = input_sampler.getInputBatch(5)\n",
        "print(\"x_input = \\n\", x_input)\n",
        "\n",
        "nn = NeuralNetwork(input_dimensions, output_dimensions, [num_hidden_neurons])\n",
        "\n",
        "print(\"\\nweights = \\n\", nn.layers[0].weights)\n",
        "print(\"\\nbias = \\n\", nn.layers[0].bias)\n",
        "\n",
        "print(\"\\nhidden_layer_output = \\n\", nn.layers[0].forward(x_input))\n",
        "\n",
        "print(\"\\nweights = \\n\", nn.layers[1].weights)\n",
        "print(\"\\nbias = \\n\", nn.layers[1].bias)\n",
        "\n",
        "print(\"\\ny_pred = \\n\", nn.forward(x_input))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yxlt8YD7Xrd_"
      },
      "source": [
        "# Recap\n",
        "\n",
        "This wraps up this notebook. Just to recap we learned -\n",
        "\n",
        "\n",
        "1.   Artificial Neuron and its mathematical representation\n",
        "2.   How to code a\n",
        "    \n",
        "        *   Neuron processing single 1-D input\n",
        "        *   Neuron processing multiple 1-D input (Batch Inputs)\n",
        "        *   Neuron processing multiple n-D inputs\n",
        "        *   NeuralNetworkLayer consisting of multiple neurons\n",
        "        *   NeuralNetwork consisting of multiple hidden layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lVyZOIeimr34"
      },
      "source": [
        "# References"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5puTbYau9hpq"
      },
      "source": [
        "1.   https://aibyhand.substack.com/"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
